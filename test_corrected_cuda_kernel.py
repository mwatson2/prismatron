#!/usr/bin/env python3
"""
Test Corrected CUDA Kernel Implementation.

This script creates a comprehensive test to verify that the corrected CUDA kernel
produces identical results to the reference NumPy implementation. It tests:

1. Correctness: Exact numerical comparison with reference implementation
2. Edge cases: Boundary conditions, empty blocks, out-of-bounds access
3. Performance: Speed comparison between implementations
4. Consistency: Multiple runs should produce identical results

The test uses small, controlled patterns to ensure we can verify every computation.
"""

import logging
import sys
import time
from pathlib import Path
from typing import Dict, List, Tuple

import cupy as cp
import numpy as np

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from src.utils.single_block_sparse_tensor import SingleBlockMixedSparseTensor

logging.basicConfig(level=logging.INFO, format="%(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


def create_reference_implementation(
    sparse_values: np.ndarray,
    block_positions: np.ndarray,
    blocks_set: np.ndarray,
    target_image: np.ndarray,
    batch_size: int,
    channels: int,
    block_size: int,
) -> np.ndarray:
    """
    Reference implementation using pure NumPy operations.

    This implementation is simple and obviously correct - we'll use it
    to verify the CUDA kernel results.
    """
    height, width = target_image.shape
    result = np.zeros((batch_size, channels), dtype=np.float32)

    for led_id in range(batch_size):
        for channel_id in range(channels):
            # Skip if block not set
            if not blocks_set[led_id, channel_id]:
                result[led_id, channel_id] = 0.0
                continue

            # Get block position
            top_row = block_positions[led_id, channel_id, 0]
            top_col = block_positions[led_id, channel_id, 1]

            # Extract target block
            target_block = np.zeros((block_size, block_size), dtype=np.float32)
            for block_row in range(block_size):
                for block_col in range(block_size):
                    global_row = top_row + block_row
                    global_col = top_col + block_col

                    if 0 <= global_row < height and 0 <= global_col < width:
                        target_block[block_row, block_col] = target_image[
                            global_row, global_col
                        ]
                    # else remains 0.0 (out of bounds)

            # Get sparse block
            sparse_block = sparse_values[led_id, channel_id]

            # Compute dot product
            dot_product = np.sum(sparse_block * target_block)
            result[led_id, channel_id] = dot_product

    return result


def create_test_tensor(
    led_count: int = 10, block_size: int = 32, add_edge_cases: bool = True
) -> Tuple[SingleBlockMixedSparseTensor, Dict]:
    """Create test tensor with known patterns for verification."""
    logger.info(
        f"Creating test tensor with {led_count} LEDs, {block_size}x{block_size} blocks..."
    )

    # Scale image size based on LED count and block size to avoid excessive overlap
    base_height, base_width = 100, 120
    scale_factor = max(1, int(np.sqrt(led_count / 10)))  # Scale with LED count
    height = max(base_height, block_size * 2, base_height * scale_factor)
    width = max(base_width, block_size * 2, base_width * scale_factor)
    channels = 3

    tensor = SingleBlockMixedSparseTensor(
        led_count, channels, height, width, block_size
    )

    # Create predictable test patterns
    np.random.seed(42)  # Reproducible

    test_info = {
        "led_count": led_count,
        "channels": channels,
        "block_size": block_size,
        "image_shape": (height, width),
        "patterns": [],
    }

    for led_id in range(led_count):
        for channel in range(channels):
            # Create different pattern types for testing
            pattern_type = (led_id * channels + channel) % 4

            if pattern_type == 0:
                # Simple constant pattern
                pattern = np.full((block_size, block_size), 0.5, dtype=np.float32)
                top_row, top_col = 10, 10

            elif pattern_type == 1:
                # Gradient pattern
                pattern = np.zeros((block_size, block_size), dtype=np.float32)
                for i in range(block_size):
                    for j in range(block_size):
                        pattern[i, j] = (i + j) / (2 * block_size)
                top_row = min(height - block_size, (led_id * 8) % (height - block_size))
                top_col = min(width - block_size, (channel * 10) % (width - block_size))

            elif pattern_type == 2:
                # Sparse checkerboard pattern
                pattern = np.zeros((block_size, block_size), dtype=np.float32)
                pattern[::2, ::2] = 1.0  # Checkerboard
                pattern[1::2, 1::2] = 1.0
                top_row = min(
                    height - block_size, ((led_id + 1) * 7) % (height - block_size)
                )
                top_col = min(
                    width - block_size, ((channel + 1) * 15) % (width - block_size)
                )

            else:
                # Gaussian-like pattern
                y, x = np.meshgrid(
                    np.arange(block_size), np.arange(block_size), indexing="ij"
                )
                center_y, center_x = block_size // 2, block_size // 2
                pattern = np.exp(
                    -((y - center_y) ** 2 + (x - center_x) ** 2) / (block_size / 4)
                )
                top_row = min(height - block_size, (led_id * 6) % (height - block_size))
                top_col = min(width - block_size, (channel * 12) % (width - block_size))

            # Add edge cases
            if add_edge_cases and led_id == led_count - 1:
                if channel == 0:
                    # Edge case: block at image boundary
                    top_row = height - block_size
                    top_col = width - block_size
                elif channel == 1:
                    # Edge case: block at corner
                    top_row = 0
                    top_col = 0

            tensor.set_block(led_id, channel, top_row, top_col, cp.asarray(pattern))

            test_info["patterns"].append(
                {
                    "led_id": led_id,
                    "channel": channel,
                    "type": pattern_type,
                    "position": (top_row, top_col),
                    "pattern_sum": float(np.sum(pattern)),
                    "pattern_max": float(np.max(pattern)),
                }
            )

    memory_info = tensor.memory_info()
    logger.info(f"  Created {memory_info['blocks_stored']} blocks")
    logger.info(f"  Memory: {memory_info['total_mb']:.1f}MB")

    return tensor, test_info


def create_test_target_images(
    height: int, width: int, count: int = 5
) -> List[cp.ndarray]:
    """Create test target images with known patterns."""
    targets = []

    for i in range(count):
        if i == 0:
            # Uniform image
            target = np.full((height, width), 1.0, dtype=np.float32)
        elif i == 1:
            # Zero image
            target = np.zeros((height, width), dtype=np.float32)
        elif i == 2:
            # Gradient
            target = np.zeros((height, width), dtype=np.float32)
            for row in range(height):
                for col in range(width):
                    target[row, col] = (row + col) / (height + width)
        elif i == 3:
            # Checkerboard
            target = np.zeros((height, width), dtype=np.float32)
            target[::2, ::2] = 1.0
            target[1::2, 1::2] = 1.0
        else:
            # Random
            np.random.seed(i + 100)
            target = np.random.rand(height, width).astype(np.float32)

        targets.append(cp.asarray(target))

    return targets


def verify_correctness(
    tensor: SingleBlockMixedSparseTensor,
    target_images: List[cp.ndarray],
    test_info: Dict,
) -> Dict:
    """Verify that corrected CUDA kernel matches reference implementation exactly."""
    logger.info("Verifying CUDA kernel correctness against reference implementation...")

    results = {
        "tests_passed": 0,
        "tests_failed": 0,
        "max_abs_error": 0.0,
        "max_rel_error": 0.0,
        "errors": [],
    }

    for i, target_image in enumerate(target_images):
        logger.info(f"  Testing target image {i+1}/{len(target_images)}...")

        # Convert to CPU for reference implementation
        target_cpu = cp.asnumpy(target_image)
        sparse_values_cpu = cp.asnumpy(tensor.sparse_values)
        block_positions_cpu = cp.asnumpy(tensor.block_positions)
        blocks_set_cpu = cp.asnumpy(tensor.blocks_set)

        # Reference implementation
        reference_result = create_reference_implementation(
            sparse_values_cpu,
            block_positions_cpu,
            blocks_set_cpu,
            target_cpu,
            tensor.batch_size,
            tensor.channels,
            tensor.block_size,
        )

        # Test original CUDA kernel (may be incorrect)
        try:
            original_result_gpu = tensor.transpose_dot_product_cuda(target_image)
            original_result = cp.asnumpy(original_result_gpu)
            original_available = True
        except Exception as e:
            logger.warning(f"Original CUDA kernel failed: {e}")
            original_result = None
            original_available = False

        # Test corrected CUDA kernel
        try:
            corrected_result_gpu = tensor.transpose_dot_product_cuda_corrected(
                target_image
            )
            corrected_result = cp.asnumpy(corrected_result_gpu)
            corrected_available = True
        except Exception as e:
            logger.warning(f"Corrected CUDA kernel failed: {e}")
            corrected_result = None
            corrected_available = False

        # Test chunked implementation
        chunked_result_gpu = tensor.transpose_dot_product(target_image)
        chunked_result = cp.asnumpy(chunked_result_gpu)

        # Compare results
        test_passed = True

        # Compare corrected CUDA vs reference
        if corrected_available:
            abs_error = np.abs(corrected_result - reference_result)
            rel_error = np.abs(abs_error / (np.abs(reference_result) + 1e-8))

            max_abs_err = np.max(abs_error)
            max_rel_err = np.max(rel_error)

            results["max_abs_error"] = max(results["max_abs_error"], max_abs_err)
            results["max_rel_error"] = max(results["max_rel_error"], max_rel_err)

            tolerance = 1e-5  # Allow for floating point precision
            if max_abs_err > tolerance and max_rel_err > tolerance:
                test_passed = False
                results["errors"].append(
                    {
                        "test": f"target_{i}",
                        "type": "corrected_vs_reference",
                        "max_abs_error": max_abs_err,
                        "max_rel_error": max_rel_err,
                    }
                )
                logger.error(
                    f"    Corrected CUDA vs Reference: max_abs_err={max_abs_err:.2e}, max_rel_err={max_rel_err:.2e}"
                )
            else:
                logger.info(
                    f"    Corrected CUDA vs Reference: PASSED (max_abs_err={max_abs_err:.2e})"
                )

        # Compare chunked vs reference (should always match)
        abs_error_chunked = np.abs(chunked_result - reference_result)
        max_abs_err_chunked = np.max(abs_error_chunked)
        if max_abs_err_chunked > 1e-5:
            logger.warning(
                f"    Chunked vs Reference: max_abs_err={max_abs_err_chunked:.2e} (unexpected!)"
            )

        # Compare original CUDA vs reference (may fail)
        if original_available:
            abs_error_original = np.abs(original_result - reference_result)
            max_abs_err_original = np.max(abs_error_original)
            logger.info(
                f"    Original CUDA vs Reference: max_abs_err={max_abs_err_original:.2e}"
            )

        if test_passed:
            results["tests_passed"] += 1
        else:
            results["tests_failed"] += 1

    # Summary
    total_tests = len(target_images)
    logger.info(f"\\nCORRECTNESS RESULTS:")
    logger.info(f"  Tests passed: {results['tests_passed']}/{total_tests}")
    logger.info(f"  Tests failed: {results['tests_failed']}/{total_tests}")
    logger.info(f"  Max absolute error: {results['max_abs_error']:.2e}")
    logger.info(f"  Max relative error: {results['max_rel_error']:.2e}")

    if results["tests_failed"] == 0:
        logger.info("  ‚úÖ All correctness tests PASSED!")
    else:
        logger.error("  ‚ùå Some correctness tests FAILED!")
        for error in results["errors"]:
            logger.error(f"    {error}")

    return results


def benchmark_performance(
    tensor: SingleBlockMixedSparseTensor,
    target_images: List[cp.ndarray],
    num_runs: int = 10,
) -> Dict:
    """Benchmark performance of different implementations."""
    logger.info(f"Benchmarking performance with {num_runs} runs...")

    # Warm up
    _ = tensor.transpose_dot_product(target_images[0])
    try:
        _ = tensor.transpose_dot_product_cuda(target_images[0])
        original_available = True
    except Exception:
        original_available = False

    try:
        _ = tensor.transpose_dot_product_cuda_corrected(target_images[0])
        corrected_available = True
    except Exception:
        corrected_available = False

    cp.cuda.Device().synchronize()

    results = {
        "corrected_available": corrected_available,
        "original_available": original_available,
    }

    # Benchmark chunked implementation
    times_chunked = []
    for i in range(num_runs):
        cp.cuda.Device().synchronize()
        start_time = time.time()
        _ = tensor.transpose_dot_product(target_images[i % len(target_images)])
        cp.cuda.Device().synchronize()
        times_chunked.append(time.time() - start_time)

    results["chunked_time_ms"] = np.mean(times_chunked) * 1000
    results["chunked_std_ms"] = np.std(times_chunked) * 1000

    # Benchmark original CUDA kernel
    if original_available:
        times_original = []
        for i in range(num_runs):
            cp.cuda.Device().synchronize()
            start_time = time.time()
            _ = tensor.transpose_dot_product_cuda(target_images[i % len(target_images)])
            cp.cuda.Device().synchronize()
            times_original.append(time.time() - start_time)

        results["original_time_ms"] = np.mean(times_original) * 1000
        results["original_std_ms"] = np.std(times_original) * 1000
        results["original_speedup"] = (
            results["chunked_time_ms"] / results["original_time_ms"]
        )

    # Benchmark corrected CUDA kernel
    if corrected_available:
        times_corrected = []
        for i in range(num_runs):
            cp.cuda.Device().synchronize()
            start_time = time.time()
            _ = tensor.transpose_dot_product_cuda_corrected(
                target_images[i % len(target_images)]
            )
            cp.cuda.Device().synchronize()
            times_corrected.append(time.time() - start_time)

        results["corrected_time_ms"] = np.mean(times_corrected) * 1000
        results["corrected_std_ms"] = np.std(times_corrected) * 1000
        results["corrected_speedup"] = (
            results["chunked_time_ms"] / results["corrected_time_ms"]
        )

        if original_available:
            results["corrected_vs_original"] = (
                results["original_time_ms"] / results["corrected_time_ms"]
            )

    # Print results
    logger.info(f"\\nPERFORMANCE RESULTS:")
    logger.info(
        f"  Chunked:          {results['chunked_time_ms']:.2f} ¬± {results['chunked_std_ms']:.2f} ms"
    )

    if original_available:
        logger.info(
            f"  Original CUDA:    {results['original_time_ms']:.2f} ¬± {results['original_std_ms']:.2f} ms ({results['original_speedup']:.2f}x)"
        )
    else:
        logger.info(f"  Original CUDA:    Not available")

    if corrected_available:
        logger.info(
            f"  Corrected CUDA:   {results['corrected_time_ms']:.2f} ¬± {results['corrected_std_ms']:.2f} ms ({results['corrected_speedup']:.2f}x)"
        )
        if original_available:
            if results["corrected_vs_original"] > 1:
                logger.info(
                    f"  Corrected vs Original: {results['corrected_vs_original']:.2f}x faster"
                )
            else:
                logger.info(
                    f"  Corrected vs Original: {1/results['corrected_vs_original']:.2f}x slower"
                )
    else:
        logger.info(f"  Corrected CUDA:   Not available")

    return results


def test_edge_cases(tensor: SingleBlockMixedSparseTensor) -> Dict:
    """Test edge cases and boundary conditions."""
    logger.info("Testing edge cases and boundary conditions...")

    results = {"tests_passed": 0, "tests_failed": 0, "errors": []}

    # Test 1: Empty target image
    try:
        empty_target = cp.zeros((tensor.height, tensor.width), dtype=cp.float32)
        ref_result = tensor.transpose_dot_product(empty_target)
        cuda_result = tensor.transpose_dot_product_cuda_corrected(empty_target)

        if cp.allclose(ref_result, cuda_result, atol=1e-6):
            results["tests_passed"] += 1
            logger.info("  ‚úÖ Empty target image test passed")
        else:
            results["tests_failed"] += 1
            results["errors"].append("Empty target image test failed")
            logger.error("  ‚ùå Empty target image test failed")
    except Exception as e:
        results["tests_failed"] += 1
        results["errors"].append(f"Empty target image test error: {e}")
        logger.error(f"  ‚ùå Empty target image test error: {e}")

    # Test 2: Very large values
    try:
        large_target = cp.full((tensor.height, tensor.width), 1e6, dtype=cp.float32)
        ref_result = tensor.transpose_dot_product(large_target)
        cuda_result = tensor.transpose_dot_product_cuda_corrected(large_target)

        if cp.allclose(ref_result, cuda_result, rtol=1e-5):
            results["tests_passed"] += 1
            logger.info("  ‚úÖ Large values test passed")
        else:
            results["tests_failed"] += 1
            results["errors"].append("Large values test failed")
            logger.error("  ‚ùå Large values test failed")
    except Exception as e:
        results["tests_failed"] += 1
        results["errors"].append(f"Large values test error: {e}")
        logger.error(f"  ‚ùå Large values test error: {e}")

    # Test 3: Consistency (multiple runs should give identical results)
    try:
        test_target = cp.random.rand(tensor.height, tensor.width).astype(cp.float32)

        results_list = []
        for _ in range(5):
            cuda_result = tensor.transpose_dot_product_cuda_corrected(test_target)
            results_list.append(cuda_result)

        # Check all results are identical
        consistent = True
        for i in range(1, len(results_list)):
            if not cp.array_equal(results_list[0], results_list[i]):
                consistent = False
                break

        if consistent:
            results["tests_passed"] += 1
            logger.info("  ‚úÖ Consistency test passed")
        else:
            results["tests_failed"] += 1
            results["errors"].append("Consistency test failed")
            logger.error("  ‚ùå Consistency test failed")
    except Exception as e:
        results["tests_failed"] += 1
        results["errors"].append(f"Consistency test error: {e}")
        logger.error(f"  ‚ùå Consistency test error: {e}")

    return results


def main():
    """Run comprehensive test suite for corrected CUDA kernel."""
    logger.info("Testing Corrected CUDA Kernel Implementation")
    logger.info("=" * 60)

    # Test different configurations
    test_configs = [
        {"led_count": 10, "block_size": 16},  # Small test
        {"led_count": 25, "block_size": 32},  # Medium test
        {"led_count": 50, "block_size": 48},  # Larger test
        {"led_count": 100, "block_size": 64},  # Large test
        {"led_count": 250, "block_size": 72},  # Very large test
        {"led_count": 500, "block_size": 96},  # Production test
        {"led_count": 1000, "block_size": 96},  # Full production test
    ]

    all_results = []

    for config in test_configs:
        logger.info(f"\\n{'='*40}")
        logger.info(
            f"Testing: {config['led_count']} LEDs, {config['block_size']}x{config['block_size']} blocks"
        )
        logger.info(f"{'='*40}")

        # Create test tensor
        tensor, test_info = create_test_tensor(
            led_count=config["led_count"], block_size=config["block_size"]
        )

        # Create test target images
        target_images = create_test_target_images(tensor.height, tensor.width, count=5)

        # Run tests - skip detailed correctness for large configs (already verified)
        if config["led_count"] <= 50:
            correctness_results = verify_correctness(tensor, target_images, test_info)
            edge_case_results = test_edge_cases(tensor)
        else:
            logger.info(
                "Skipping detailed correctness tests for large config (already verified)"
            )
            # Quick correctness check
            test_target = target_images[0]
            ref_result = tensor.transpose_dot_product(test_target)
            cuda_result = tensor.transpose_dot_product_cuda_corrected(test_target)
            max_error = float(cp.max(cp.abs(ref_result - cuda_result)))

            correctness_results = {
                "tests_passed": 1 if max_error < 1e-4 else 0,
                "tests_failed": 0 if max_error < 1e-4 else 1,
                "max_abs_error": max_error,
                "errors": []
                if max_error < 1e-4
                else [f"Quick check failed: {max_error}"],
            }
            edge_case_results = {"tests_passed": 0, "tests_failed": 0, "errors": []}

            if max_error < 1e-4:
                logger.info(
                    f"  ‚úÖ Quick correctness check passed (max_error={max_error:.2e})"
                )
            else:
                logger.error(
                    f"  ‚ùå Quick correctness check failed (max_error={max_error:.2e})"
                )

        config_results = {
            "config": config,
            "correctness": correctness_results,
            "performance": benchmark_performance(tensor, target_images),
            "edge_cases": edge_case_results,
        }

        all_results.append(config_results)

    # Final summary
    logger.info(f"\\n{'='*60}")
    logger.info("FINAL TEST SUMMARY")
    logger.info(f"{'='*60}")

    total_correctness_passed = sum(
        r["correctness"]["tests_passed"] for r in all_results
    )
    total_correctness_failed = sum(
        r["correctness"]["tests_failed"] for r in all_results
    )
    total_edge_passed = sum(r["edge_cases"]["tests_passed"] for r in all_results)
    total_edge_failed = sum(r["edge_cases"]["tests_failed"] for r in all_results)

    logger.info(
        f"Correctness tests: {total_correctness_passed} passed, {total_correctness_failed} failed"
    )
    logger.info(
        f"Edge case tests:   {total_edge_passed} passed, {total_edge_failed} failed"
    )

    # Performance summary
    corrected_available = any(
        r["performance"]["corrected_available"] for r in all_results
    )
    if corrected_available:
        logger.info(f"\\nCorrected CUDA kernel performance:")
        for r in all_results:
            if r["performance"]["corrected_available"]:
                config = r["config"]
                perf = r["performance"]
                logger.info(
                    f"  {config['led_count']} LEDs: {perf['corrected_time_ms']:.2f}ms ({perf['corrected_speedup']:.2f}x speedup)"
                )

    # Overall result
    if total_correctness_failed == 0 and total_edge_failed == 0:
        logger.info(
            f"\\nüéâ ALL TESTS PASSED! Corrected CUDA kernel is working correctly."
        )
        return 0
    else:
        logger.error(f"\\n‚ùå SOME TESTS FAILED! Check the errors above.")
        return 1


if __name__ == "__main__":
    sys.exit(main())
