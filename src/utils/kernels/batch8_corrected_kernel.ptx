//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-34714021
// Cuda compilation tools, release 12.6, V12.6.68
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_80
.address_size 64

	// .globl	batch8_symmetric_block_pair_multiply_wmma
// _ZZ41batch8_symmetric_block_pair_multiply_wmmaE8shared_a has been demoted
// _ZZ41batch8_symmetric_block_pair_multiply_wmmaE8shared_b has been demoted
// _ZZ41batch8_symmetric_block_pair_multiply_wmmaE13block_results has been demoted
// _ZZ41batch8_symmetric_block_pair_multiply_wmmaE6temp_c has been demoted

.visible .entry batch8_symmetric_block_pair_multiply_wmma(
	.param .u64 batch8_symmetric_block_pair_multiply_wmma_param_0,
	.param .u64 batch8_symmetric_block_pair_multiply_wmma_param_1,
	.param .u64 batch8_symmetric_block_pair_multiply_wmma_param_2,
	.param .u64 batch8_symmetric_block_pair_multiply_wmma_param_3,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_param_4,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_param_5,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_param_6,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_param_7,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_param_8
)
{
	.reg .pred 	%p<161>;
	.reg .b16 	%rs<176>;
	.reg .f32 	%f<160>;
	.reg .b32 	%r<369>;
	.reg .b64 	%rd<446>;
	// demoted variable
	.shared .align 2 .b8 _ZZ41batch8_symmetric_block_pair_multiply_wmmaE8shared_a[1024];
	// demoted variable
	.shared .align 2 .b8 _ZZ41batch8_symmetric_block_pair_multiply_wmmaE8shared_b[256];
	// demoted variable
	.shared .align 4 .b8 _ZZ41batch8_symmetric_block_pair_multiply_wmmaE13block_results[1024];
	// demoted variable
	.shared .align 4 .b8 _ZZ41batch8_symmetric_block_pair_multiply_wmmaE6temp_c[1024];

	ld.param.u64 	%rd180, [batch8_symmetric_block_pair_multiply_wmma_param_0];
	ld.param.u64 	%rd182, [batch8_symmetric_block_pair_multiply_wmma_param_1];
	ld.param.u64 	%rd183, [batch8_symmetric_block_pair_multiply_wmma_param_2];
	ld.param.u32 	%r68, [batch8_symmetric_block_pair_multiply_wmma_param_5];
	ld.param.u32 	%r69, [batch8_symmetric_block_pair_multiply_wmma_param_6];
	ld.param.u32 	%r70, [batch8_symmetric_block_pair_multiply_wmma_param_7];
	ld.param.u32 	%r71, [batch8_symmetric_block_pair_multiply_wmma_param_8];
	cvta.to.global.u64 	%rd1, %rd182;
	cvta.to.global.u64 	%rd2, %rd183;
	cvta.to.global.u64 	%rd3, %rd180;
	mov.u32 	%r1, %tid.x;
	and.b32  	%r2, %r1, 31;
	mov.u32 	%r3, %ctaid.x;
	setp.ge.s32 	%p1, %r3, %r68;
	mov.u32 	%r4, %ctaid.y;
	shl.b32 	%r5, %r4, 1;
	setp.ge.s32 	%p2, %r5, %r69;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_207;

	shl.b32 	%r72, %r2, 5;
	mov.u32 	%r73, _ZZ41batch8_symmetric_block_pair_multiply_wmmaE13block_results;
	add.s32 	%r6, %r73, %r72;
	mov.u32 	%r360, 0;
	st.shared.u32 	[%r6], %r360;
	st.shared.u32 	[%r6+4], %r360;
	st.shared.u32 	[%r6+8], %r360;
	st.shared.u32 	[%r6+12], %r360;
	st.shared.u32 	[%r6+16], %r360;
	st.shared.u32 	[%r6+20], %r360;
	st.shared.u32 	[%r6+24], %r360;
	st.shared.u32 	[%r6+28], %r360;
	add.s32 	%r7, %r5, 1;
	bar.warp.sync 	-1;
	setp.lt.s32 	%p4, %r69, 1;
	@%p4 bra 	$L__BB0_205;

	mul.lo.s32 	%r8, %r3, %r70;
	add.s32 	%r9, %r70, -1;
	and.b32  	%r11, %r70, 3;
	sub.s32 	%r12, %r70, %r11;
	and.b32  	%r76, %r1, 7;
	mad.lo.s32 	%r77, %r76, %r68, %r3;
	mul.lo.s32 	%r13, %r77, %r71;
	shl.b32 	%r78, %r2, 1;
	mov.u32 	%r79, _ZZ41batch8_symmetric_block_pair_multiply_wmmaE8shared_b;
	add.s32 	%r14, %r79, %r78;
	or.b32  	%r80, %r2, 32;
	or.b32  	%r81, %r2, 64;
	or.b32  	%r82, %r2, 96;
	mov.u32 	%r83, _ZZ41batch8_symmetric_block_pair_multiply_wmmaE8shared_a;
	add.s32 	%r18, %r83, %r78;
	shr.u32 	%r84, %r80, 4;
	shr.u32 	%r85, %r81, 4;
	and.b32  	%r86, %r1, 15;
	shl.b32 	%r87, %r1, 4;
	and.b32  	%r88, %r87, 240;
	shr.u32 	%r89, %r2, 4;
	or.b32  	%r20, %r88, %r89;
	shr.u32 	%r90, %r82, 4;
	and.b32  	%r91, %r80, 48;
	or.b32  	%r92, %r91, %r86;
	cvt.u64.u32 	%rd4, %r92;
	or.b32  	%r93, %r88, %r84;
	cvt.u64.u32 	%rd5, %r93;
	or.b32  	%r94, %r2, 128;
	shr.u32 	%r95, %r94, 4;
	and.b32  	%r96, %r81, 80;
	or.b32  	%r97, %r96, %r86;
	cvt.u64.u32 	%rd6, %r97;
	or.b32  	%r98, %r88, %r85;
	cvt.u64.u32 	%rd7, %r98;
	or.b32  	%r99, %r2, 160;
	shr.u32 	%r100, %r99, 4;
	and.b32  	%r101, %r82, 112;
	or.b32  	%r102, %r101, %r86;
	cvt.u64.u32 	%rd8, %r102;
	or.b32  	%r103, %r88, %r90;
	cvt.u64.u32 	%rd9, %r103;
	or.b32  	%r104, %r2, 192;
	shr.u32 	%r105, %r104, 4;
	and.b32  	%r106, %r94, 144;
	or.b32  	%r107, %r106, %r86;
	cvt.u64.u32 	%rd10, %r107;
	or.b32  	%r108, %r88, %r95;
	cvt.u64.u32 	%rd11, %r108;
	or.b32  	%r109, %r2, 224;
	shr.u32 	%r110, %r109, 4;
	and.b32  	%r111, %r99, 176;
	or.b32  	%r112, %r111, %r86;
	cvt.u64.u32 	%rd12, %r112;
	or.b32  	%r113, %r88, %r100;
	cvt.u64.u32 	%rd13, %r113;
	and.b32  	%r114, %r104, 208;
	or.b32  	%r115, %r114, %r86;
	cvt.u64.u32 	%rd14, %r115;
	or.b32  	%r116, %r88, %r105;
	cvt.u64.u32 	%rd15, %r116;
	or.b32  	%r117, %r2, 288;
	shr.u32 	%r118, %r117, 4;
	or.b32  	%r119, %r2, 256;
	shr.u32 	%r120, %r119, 4;
	add.s32 	%r121, %r120, -16;
	and.b32  	%r122, %r109, 240;
	or.b32  	%r123, %r122, %r86;
	cvt.u64.u32 	%rd16, %r123;
	or.b32  	%r124, %r88, %r110;
	cvt.u64.u32 	%rd17, %r124;
	or.b32  	%r125, %r2, 320;
	shr.u32 	%r126, %r125, 4;
	bfi.b32 	%r21, %r121, %r86, 4, 28;
	add.s32 	%r22, %r88, %r121;
	or.b32  	%r127, %r2, 352;
	shr.u32 	%r128, %r127, 4;
	add.s32 	%r129, %r118, -16;
	or.b32  	%r130, %r2, 384;
	shr.u32 	%r131, %r130, 4;
	add.s32 	%r132, %r126, -16;
	bfi.b32 	%r133, %r129, %r86, 4, 28;
	cvt.s64.s32 	%rd18, %r133;
	add.s32 	%r134, %r88, %r129;
	cvt.s64.s32 	%rd19, %r134;
	or.b32  	%r135, %r2, 416;
	shr.u32 	%r136, %r135, 4;
	add.s32 	%r137, %r128, -16;
	bfi.b32 	%r138, %r132, %r86, 4, 28;
	cvt.s64.s32 	%rd20, %r138;
	add.s32 	%r139, %r88, %r132;
	cvt.s64.s32 	%rd21, %r139;
	or.b32  	%r140, %r2, 448;
	shr.u32 	%r141, %r140, 4;
	add.s32 	%r142, %r131, -16;
	bfi.b32 	%r143, %r137, %r86, 4, 28;
	cvt.s64.s32 	%rd22, %r143;
	add.s32 	%r144, %r88, %r137;
	cvt.s64.s32 	%rd23, %r144;
	or.b32  	%r145, %r2, 480;
	shr.u32 	%r146, %r145, 4;
	add.s32 	%r147, %r136, -16;
	bfi.b32 	%r148, %r142, %r86, 4, 28;
	cvt.s64.s32 	%rd24, %r148;
	add.s32 	%r149, %r88, %r142;
	cvt.s64.s32 	%rd25, %r149;
	mov.u32 	%r151, _ZZ41batch8_symmetric_block_pair_multiply_wmmaE6temp_c;
	add.s32 	%r23, %r151, %r72;
	add.s32 	%r152, %r141, -16;
	bfi.b32 	%r153, %r147, %r86, 4, 28;
	cvt.s64.s32 	%rd26, %r153;
	add.s32 	%r154, %r88, %r147;
	cvt.s64.s32 	%rd27, %r154;
	add.s32 	%r155, %r146, -16;
	bfi.b32 	%r156, %r152, %r86, 4, 28;
	cvt.s64.s32 	%rd28, %r156;
	add.s32 	%r157, %r88, %r152;
	cvt.s64.s32 	%rd29, %r157;
	bfi.b32 	%r158, %r155, %r86, 4, 28;
	cvt.s64.s32 	%rd30, %r158;
	add.s32 	%r159, %r88, %r155;
	cvt.s64.s32 	%rd31, %r159;
	shl.b64 	%rd300, %rd5, 2;
	shl.b64 	%rd302, %rd4, 2;
	shl.b64 	%rd304, %rd7, 2;
	shl.b64 	%rd306, %rd6, 2;
	shl.b64 	%rd308, %rd9, 2;
	shl.b64 	%rd310, %rd8, 2;
	shl.b64 	%rd312, %rd11, 2;
	shl.b64 	%rd314, %rd10, 2;
	shl.b64 	%rd316, %rd13, 2;
	shl.b64 	%rd318, %rd12, 2;
	shl.b64 	%rd320, %rd15, 2;
	shl.b64 	%rd322, %rd14, 2;
	shl.b64 	%rd324, %rd17, 2;
	shl.b64 	%rd326, %rd16, 2;
	shl.b64 	%rd330, %rd19, 2;
	shl.b64 	%rd332, %rd18, 2;
	shl.b64 	%rd334, %rd21, 2;
	shl.b64 	%rd336, %rd20, 2;
	shl.b64 	%rd338, %rd23, 2;
	shl.b64 	%rd340, %rd22, 2;
	shl.b64 	%rd342, %rd25, 2;
	shl.b64 	%rd344, %rd24, 2;
	shl.b64 	%rd346, %rd27, 2;
	shl.b64 	%rd348, %rd26, 2;
	shl.b64 	%rd350, %rd29, 2;
	shl.b64 	%rd352, %rd28, 2;
	shl.b64 	%rd354, %rd31, 2;
	shl.b64 	%rd356, %rd30, 2;

$L__BB0_3:
	setp.lt.s32 	%p5, %r70, 1;
	mov.u16 	%rs174, 0;
	mov.u64 	%rd442, 0;
	mov.u64 	%rd443, %rd442;
	mov.u64 	%rd444, %rd442;
	mov.u64 	%rd445, %rd442;
	mov.u16 	%rs175, %rs174;
	@%p5 bra 	$L__BB0_122;

	setp.lt.s32 	%p6, %r7, %r69;
	@%p6 bra 	$L__BB0_46;
	bra.uni 	$L__BB0_5;

$L__BB0_46:
	setp.lt.u32 	%p47, %r9, 3;
	mov.u64 	%rd445, 0;
	mov.u16 	%rs175, 0;
	mov.u32 	%r366, 0;
	mov.u16 	%rs174, %rs175;
	mov.u64 	%rd444, %rd445;
	mov.u64 	%rd443, %rd445;
	mov.u64 	%rd442, %rd445;
	@%p47 bra 	$L__BB0_89;

	mov.u64 	%rd445, 0;
	mov.u16 	%rs175, 0;
	mov.u32 	%r366, 0;
	mov.u32 	%r365, %r12;

$L__BB0_48:
	mul.wide.s32 	%rd245, %r366, 4;
	add.s64 	%rd84, %rd1, %rd245;
	ld.global.u32 	%r44, [%rd84];
	setp.ne.s64 	%p48, %rd443, 0;
	@%p48 bra 	$L__BB0_53;

	add.s32 	%r206, %r44, %r5;
	setp.eq.s32 	%p49, %r206, %r360;
	@%p49 bra 	$L__BB0_52;
	bra.uni 	$L__BB0_50;

$L__BB0_52:
	add.s32 	%r210, %r366, %r8;
	shl.b32 	%r211, %r210, 8;
	mul.wide.s32 	%rd248, %r211, 4;
	add.s64 	%rd442, %rd3, %rd248;
	add.s64 	%rd443, %rd180, %rd248;
	mov.u16 	%rs174, 0;
	bra.uni 	$L__BB0_53;

$L__BB0_50:
	add.s32 	%r207, %r44, %r360;
	setp.ne.s32 	%p50, %r207, %r5;
	setp.lt.s32 	%p51, %r44, 1;
	mov.u64 	%rd443, 0;
	or.pred  	%p52, %p51, %p50;
	@%p52 bra 	$L__BB0_53;

	add.s32 	%r208, %r366, %r8;
	shl.b32 	%r209, %r208, 8;
	mul.wide.s32 	%rd247, %r209, 4;
	add.s64 	%rd442, %rd3, %rd247;
	add.s64 	%rd443, %rd180, %rd247;
	mov.u16 	%rs174, 1;

$L__BB0_53:
	setp.ne.s64 	%p53, %rd445, 0;
	@%p53 bra 	$L__BB0_58;

	add.s32 	%r212, %r44, %r7;
	setp.eq.s32 	%p54, %r212, %r360;
	@%p54 bra 	$L__BB0_57;
	bra.uni 	$L__BB0_55;

$L__BB0_57:
	add.s32 	%r216, %r366, %r8;
	shl.b32 	%r217, %r216, 8;
	mul.wide.s32 	%rd251, %r217, 4;
	add.s64 	%rd444, %rd3, %rd251;
	add.s64 	%rd445, %rd180, %rd251;
	mov.u16 	%rs175, 0;
	bra.uni 	$L__BB0_58;

$L__BB0_55:
	add.s32 	%r213, %r44, %r360;
	setp.ne.s32 	%p55, %r213, %r7;
	setp.lt.s32 	%p56, %r44, 1;
	mov.u64 	%rd445, 0;
	or.pred  	%p57, %p56, %p55;
	@%p57 bra 	$L__BB0_58;

	add.s32 	%r214, %r366, %r8;
	shl.b32 	%r215, %r214, 8;
	mul.wide.s32 	%rd250, %r215, 4;
	add.s64 	%rd444, %rd3, %rd250;
	add.s64 	%rd445, %rd180, %rd250;
	mov.u16 	%rs175, 1;

$L__BB0_58:
	ld.global.u32 	%r46, [%rd84+4];
	setp.ne.s64 	%p58, %rd443, 0;
	@%p58 bra 	$L__BB0_63;

	add.s32 	%r218, %r46, %r5;
	setp.eq.s32 	%p59, %r218, %r360;
	@%p59 bra 	$L__BB0_62;
	bra.uni 	$L__BB0_60;

$L__BB0_62:
	add.s32 	%r340, %r366, 1;
	add.s32 	%r222, %r340, %r8;
	shl.b32 	%r223, %r222, 8;
	mul.wide.s32 	%rd254, %r223, 4;
	add.s64 	%rd442, %rd3, %rd254;
	add.s64 	%rd443, %rd180, %rd254;
	mov.u16 	%rs174, 0;
	bra.uni 	$L__BB0_63;

$L__BB0_60:
	add.s32 	%r219, %r46, %r360;
	setp.ne.s32 	%p60, %r219, %r5;
	setp.lt.s32 	%p61, %r46, 1;
	mov.u64 	%rd443, 0;
	or.pred  	%p62, %p61, %p60;
	@%p62 bra 	$L__BB0_63;

	add.s32 	%r339, %r366, 1;
	add.s32 	%r220, %r339, %r8;
	shl.b32 	%r221, %r220, 8;
	mul.wide.s32 	%rd253, %r221, 4;
	add.s64 	%rd442, %rd3, %rd253;
	add.s64 	%rd443, %rd180, %rd253;
	mov.u16 	%rs174, 1;

$L__BB0_63:
	setp.ne.s64 	%p63, %rd445, 0;
	@%p63 bra 	$L__BB0_68;

	add.s32 	%r224, %r46, %r7;
	setp.eq.s32 	%p64, %r224, %r360;
	@%p64 bra 	$L__BB0_67;
	bra.uni 	$L__BB0_65;

$L__BB0_67:
	add.s32 	%r338, %r366, 1;
	add.s32 	%r228, %r338, %r8;
	shl.b32 	%r229, %r228, 8;
	mul.wide.s32 	%rd257, %r229, 4;
	add.s64 	%rd444, %rd3, %rd257;
	add.s64 	%rd445, %rd180, %rd257;
	mov.u16 	%rs175, 0;
	bra.uni 	$L__BB0_68;

$L__BB0_65:
	add.s32 	%r225, %r46, %r360;
	setp.ne.s32 	%p65, %r225, %r7;
	setp.lt.s32 	%p66, %r46, 1;
	mov.u64 	%rd445, 0;
	or.pred  	%p67, %p66, %p65;
	@%p67 bra 	$L__BB0_68;

	add.s32 	%r337, %r366, 1;
	add.s32 	%r226, %r337, %r8;
	shl.b32 	%r227, %r226, 8;
	mul.wide.s32 	%rd256, %r227, 4;
	add.s64 	%rd444, %rd3, %rd256;
	add.s64 	%rd445, %rd180, %rd256;
	mov.u16 	%rs175, 1;

$L__BB0_68:
	ld.global.u32 	%r48, [%rd84+8];
	setp.ne.s64 	%p68, %rd443, 0;
	@%p68 bra 	$L__BB0_73;

	add.s32 	%r230, %r48, %r5;
	setp.eq.s32 	%p69, %r230, %r360;
	@%p69 bra 	$L__BB0_72;
	bra.uni 	$L__BB0_70;

$L__BB0_72:
	add.s32 	%r344, %r366, 2;
	add.s32 	%r234, %r344, %r8;
	shl.b32 	%r235, %r234, 8;
	mul.wide.s32 	%rd260, %r235, 4;
	add.s64 	%rd442, %rd3, %rd260;
	add.s64 	%rd443, %rd180, %rd260;
	mov.u16 	%rs174, 0;
	bra.uni 	$L__BB0_73;

$L__BB0_70:
	add.s32 	%r231, %r48, %r360;
	setp.ne.s32 	%p70, %r231, %r5;
	setp.lt.s32 	%p71, %r48, 1;
	mov.u64 	%rd443, 0;
	or.pred  	%p72, %p71, %p70;
	@%p72 bra 	$L__BB0_73;

	add.s32 	%r343, %r366, 2;
	add.s32 	%r232, %r343, %r8;
	shl.b32 	%r233, %r232, 8;
	mul.wide.s32 	%rd259, %r233, 4;
	add.s64 	%rd442, %rd3, %rd259;
	add.s64 	%rd443, %rd180, %rd259;
	mov.u16 	%rs174, 1;

$L__BB0_73:
	setp.ne.s64 	%p73, %rd445, 0;
	@%p73 bra 	$L__BB0_78;

	add.s32 	%r236, %r48, %r7;
	setp.eq.s32 	%p74, %r236, %r360;
	@%p74 bra 	$L__BB0_77;
	bra.uni 	$L__BB0_75;

$L__BB0_77:
	add.s32 	%r342, %r366, 2;
	add.s32 	%r240, %r342, %r8;
	shl.b32 	%r241, %r240, 8;
	mul.wide.s32 	%rd263, %r241, 4;
	add.s64 	%rd444, %rd3, %rd263;
	add.s64 	%rd445, %rd180, %rd263;
	mov.u16 	%rs175, 0;
	bra.uni 	$L__BB0_78;

$L__BB0_75:
	add.s32 	%r237, %r48, %r360;
	setp.ne.s32 	%p75, %r237, %r7;
	setp.lt.s32 	%p76, %r48, 1;
	mov.u64 	%rd445, 0;
	or.pred  	%p77, %p76, %p75;
	@%p77 bra 	$L__BB0_78;

	add.s32 	%r341, %r366, 2;
	add.s32 	%r238, %r341, %r8;
	shl.b32 	%r239, %r238, 8;
	mul.wide.s32 	%rd262, %r239, 4;
	add.s64 	%rd444, %rd3, %rd262;
	add.s64 	%rd445, %rd180, %rd262;
	mov.u16 	%rs175, 1;

$L__BB0_78:
	ld.global.u32 	%r50, [%rd84+12];
	setp.ne.s64 	%p78, %rd443, 0;
	@%p78 bra 	$L__BB0_83;

	add.s32 	%r242, %r50, %r5;
	setp.eq.s32 	%p79, %r242, %r360;
	@%p79 bra 	$L__BB0_82;
	bra.uni 	$L__BB0_80;

$L__BB0_82:
	add.s32 	%r348, %r366, 3;
	add.s32 	%r246, %r348, %r8;
	shl.b32 	%r247, %r246, 8;
	mul.wide.s32 	%rd266, %r247, 4;
	add.s64 	%rd442, %rd3, %rd266;
	add.s64 	%rd443, %rd180, %rd266;
	mov.u16 	%rs174, 0;
	bra.uni 	$L__BB0_83;

$L__BB0_80:
	add.s32 	%r243, %r50, %r360;
	setp.ne.s32 	%p80, %r243, %r5;
	setp.lt.s32 	%p81, %r50, 1;
	mov.u64 	%rd443, 0;
	or.pred  	%p82, %p81, %p80;
	@%p82 bra 	$L__BB0_83;

	add.s32 	%r347, %r366, 3;
	add.s32 	%r244, %r347, %r8;
	shl.b32 	%r245, %r244, 8;
	mul.wide.s32 	%rd265, %r245, 4;
	add.s64 	%rd442, %rd3, %rd265;
	add.s64 	%rd443, %rd180, %rd265;
	mov.u16 	%rs174, 1;

$L__BB0_83:
	setp.ne.s64 	%p83, %rd445, 0;
	@%p83 bra 	$L__BB0_88;

	add.s32 	%r248, %r50, %r7;
	setp.eq.s32 	%p84, %r248, %r360;
	@%p84 bra 	$L__BB0_87;
	bra.uni 	$L__BB0_85;

$L__BB0_87:
	add.s32 	%r346, %r366, 3;
	add.s32 	%r252, %r346, %r8;
	shl.b32 	%r253, %r252, 8;
	mul.wide.s32 	%rd269, %r253, 4;
	add.s64 	%rd444, %rd3, %rd269;
	add.s64 	%rd445, %rd180, %rd269;
	mov.u16 	%rs175, 0;
	bra.uni 	$L__BB0_88;

$L__BB0_85:
	add.s32 	%r249, %r50, %r360;
	setp.ne.s32 	%p85, %r249, %r7;
	setp.lt.s32 	%p86, %r50, 1;
	mov.u64 	%rd445, 0;
	or.pred  	%p87, %p86, %p85;
	@%p87 bra 	$L__BB0_88;

	add.s32 	%r345, %r366, 3;
	add.s32 	%r250, %r345, %r8;
	shl.b32 	%r251, %r250, 8;
	mul.wide.s32 	%rd268, %r251, 4;
	add.s64 	%rd444, %rd3, %rd268;
	add.s64 	%rd445, %rd180, %rd268;
	mov.u16 	%rs175, 1;

$L__BB0_88:
	add.s32 	%r366, %r366, 4;
	add.s32 	%r365, %r365, -4;
	setp.ne.s32 	%p88, %r365, 0;
	@%p88 bra 	$L__BB0_48;

$L__BB0_89:
	setp.eq.s32 	%p89, %r11, 0;
	@%p89 bra 	$L__BB0_122;

	mul.wide.s32 	%rd270, %r366, 4;
	add.s64 	%rd141, %rd1, %rd270;
	ld.global.u32 	%r54, [%rd141];
	setp.ne.s64 	%p90, %rd443, 0;
	@%p90 bra 	$L__BB0_95;

	add.s32 	%r254, %r54, %r5;
	setp.eq.s32 	%p91, %r254, %r360;
	@%p91 bra 	$L__BB0_94;
	bra.uni 	$L__BB0_92;

$L__BB0_94:
	add.s32 	%r258, %r366, %r8;
	shl.b32 	%r259, %r258, 8;
	mul.wide.s32 	%rd273, %r259, 4;
	add.s64 	%rd442, %rd3, %rd273;
	add.s64 	%rd443, %rd180, %rd273;
	mov.u16 	%rs174, 0;
	bra.uni 	$L__BB0_95;

$L__BB0_5:
	setp.lt.u32 	%p7, %r9, 3;
	mov.u64 	%rd443, 0;
	mov.u16 	%rs174, 0;
	mov.u32 	%r363, 0;
	mov.u64 	%rd442, %rd443;
	@%p7 bra 	$L__BB0_28;

	mov.u64 	%rd443, 0;
	mov.u16 	%rs174, 0;
	mov.u32 	%r363, 0;
	mov.u32 	%r362, %r12;

$L__BB0_7:
	mul.wide.s32 	%rd196, %r363, 4;
	add.s64 	%rd34, %rd1, %rd196;
	ld.global.u32 	%r27, [%rd34];
	setp.ne.s64 	%p8, %rd443, 0;
	@%p8 bra 	$L__BB0_12;

	add.s32 	%r162, %r27, %r5;
	setp.eq.s32 	%p9, %r162, %r360;
	@%p9 bra 	$L__BB0_11;
	bra.uni 	$L__BB0_9;

$L__BB0_11:
	add.s32 	%r166, %r363, %r8;
	shl.b32 	%r167, %r166, 8;
	mul.wide.s32 	%rd199, %r167, 4;
	add.s64 	%rd442, %rd3, %rd199;
	add.s64 	%rd443, %rd180, %rd199;
	mov.u16 	%rs174, 0;
	bra.uni 	$L__BB0_12;

$L__BB0_9:
	add.s32 	%r163, %r27, %r360;
	setp.ne.s32 	%p10, %r163, %r5;
	setp.lt.s32 	%p11, %r27, 1;
	mov.u64 	%rd443, 0;
	or.pred  	%p12, %p11, %p10;
	@%p12 bra 	$L__BB0_12;

	add.s32 	%r164, %r363, %r8;
	shl.b32 	%r165, %r164, 8;
	mul.wide.s32 	%rd198, %r165, 4;
	add.s64 	%rd442, %rd3, %rd198;
	add.s64 	%rd443, %rd180, %rd198;
	mov.u16 	%rs174, 1;

$L__BB0_12:
	ld.global.u32 	%r29, [%rd34+4];
	setp.ne.s64 	%p13, %rd443, 0;
	@%p13 bra 	$L__BB0_17;

	add.s32 	%r168, %r29, %r5;
	setp.eq.s32 	%p14, %r168, %r360;
	@%p14 bra 	$L__BB0_16;
	bra.uni 	$L__BB0_14;

$L__BB0_16:
	add.s32 	%r332, %r363, 1;
	add.s32 	%r172, %r332, %r8;
	shl.b32 	%r173, %r172, 8;
	mul.wide.s32 	%rd202, %r173, 4;
	add.s64 	%rd442, %rd3, %rd202;
	add.s64 	%rd443, %rd180, %rd202;
	mov.u16 	%rs174, 0;
	bra.uni 	$L__BB0_17;

$L__BB0_14:
	add.s32 	%r169, %r29, %r360;
	setp.ne.s32 	%p15, %r169, %r5;
	setp.lt.s32 	%p16, %r29, 1;
	mov.u64 	%rd443, 0;
	or.pred  	%p17, %p16, %p15;
	@%p17 bra 	$L__BB0_17;

	add.s32 	%r331, %r363, 1;
	add.s32 	%r170, %r331, %r8;
	shl.b32 	%r171, %r170, 8;
	mul.wide.s32 	%rd201, %r171, 4;
	add.s64 	%rd442, %rd3, %rd201;
	add.s64 	%rd443, %rd180, %rd201;
	mov.u16 	%rs174, 1;

$L__BB0_17:
	ld.global.u32 	%r31, [%rd34+8];
	setp.ne.s64 	%p18, %rd443, 0;
	@%p18 bra 	$L__BB0_22;

	add.s32 	%r174, %r31, %r5;
	setp.eq.s32 	%p19, %r174, %r360;
	@%p19 bra 	$L__BB0_21;
	bra.uni 	$L__BB0_19;

$L__BB0_21:
	add.s32 	%r334, %r363, 2;
	add.s32 	%r178, %r334, %r8;
	shl.b32 	%r179, %r178, 8;
	mul.wide.s32 	%rd205, %r179, 4;
	add.s64 	%rd442, %rd3, %rd205;
	add.s64 	%rd443, %rd180, %rd205;
	mov.u16 	%rs174, 0;
	bra.uni 	$L__BB0_22;

$L__BB0_19:
	add.s32 	%r175, %r31, %r360;
	setp.ne.s32 	%p20, %r175, %r5;
	setp.lt.s32 	%p21, %r31, 1;
	mov.u64 	%rd443, 0;
	or.pred  	%p22, %p21, %p20;
	@%p22 bra 	$L__BB0_22;

	add.s32 	%r333, %r363, 2;
	add.s32 	%r176, %r333, %r8;
	shl.b32 	%r177, %r176, 8;
	mul.wide.s32 	%rd204, %r177, 4;
	add.s64 	%rd442, %rd3, %rd204;
	add.s64 	%rd443, %rd180, %rd204;
	mov.u16 	%rs174, 1;

$L__BB0_22:
	add.s32 	%r32, %r363, 3;
	ld.global.u32 	%r33, [%rd34+12];
	setp.ne.s64 	%p23, %rd443, 0;
	@%p23 bra 	$L__BB0_27;

	add.s32 	%r180, %r33, %r5;
	setp.eq.s32 	%p24, %r180, %r360;
	@%p24 bra 	$L__BB0_26;
	bra.uni 	$L__BB0_24;

$L__BB0_26:
	add.s32 	%r184, %r32, %r8;
	shl.b32 	%r185, %r184, 8;
	mul.wide.s32 	%rd208, %r185, 4;
	add.s64 	%rd442, %rd3, %rd208;
	add.s64 	%rd443, %rd180, %rd208;
	mov.u16 	%rs174, 0;
	bra.uni 	$L__BB0_27;

$L__BB0_24:
	add.s32 	%r181, %r33, %r360;
	setp.ne.s32 	%p25, %r181, %r5;
	setp.lt.s32 	%p26, %r33, 1;
	mov.u64 	%rd443, 0;
	or.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB0_27;

	add.s32 	%r182, %r32, %r8;
	shl.b32 	%r183, %r182, 8;
	mul.wide.s32 	%rd207, %r183, 4;
	add.s64 	%rd442, %rd3, %rd207;
	add.s64 	%rd443, %rd180, %rd207;
	mov.u16 	%rs174, 1;

$L__BB0_27:
	add.s32 	%r363, %r363, 4;
	add.s32 	%r362, %r362, -4;
	setp.ne.s32 	%p28, %r362, 0;
	@%p28 bra 	$L__BB0_7;

$L__BB0_28:
	mov.u16 	%rs175, 0;
	mov.u64 	%rd444, 0;
	setp.eq.s32 	%p29, %r11, 0;
	mov.u64 	%rd445, %rd444;
	@%p29 bra 	$L__BB0_122;

	mul.wide.s32 	%rd211, %r363, 4;
	add.s64 	%rd63, %rd1, %rd211;
	ld.global.u32 	%r37, [%rd63];
	setp.ne.s64 	%p30, %rd443, 0;
	@%p30 bra 	$L__BB0_34;

	add.s32 	%r186, %r37, %r5;
	setp.eq.s32 	%p31, %r186, %r360;
	@%p31 bra 	$L__BB0_33;
	bra.uni 	$L__BB0_31;

$L__BB0_33:
	add.s32 	%r190, %r363, %r8;
	shl.b32 	%r191, %r190, 8;
	mul.wide.s32 	%rd214, %r191, 4;
	add.s64 	%rd442, %rd3, %rd214;
	add.s64 	%rd443, %rd180, %rd214;
	mov.u16 	%rs174, 0;
	bra.uni 	$L__BB0_34;

$L__BB0_92:
	add.s32 	%r255, %r54, %r360;
	setp.ne.s32 	%p92, %r255, %r5;
	setp.lt.s32 	%p93, %r54, 1;
	mov.u64 	%rd443, 0;
	or.pred  	%p94, %p93, %p92;
	@%p94 bra 	$L__BB0_95;

	add.s32 	%r256, %r366, %r8;
	shl.b32 	%r257, %r256, 8;
	mul.wide.s32 	%rd272, %r257, 4;
	add.s64 	%rd442, %rd3, %rd272;
	add.s64 	%rd443, %rd180, %rd272;
	mov.u16 	%rs174, 1;

$L__BB0_95:
	setp.ne.s64 	%p95, %rd445, 0;
	@%p95 bra 	$L__BB0_100;

	add.s32 	%r260, %r54, %r7;
	setp.eq.s32 	%p96, %r260, %r360;
	@%p96 bra 	$L__BB0_99;
	bra.uni 	$L__BB0_97;

$L__BB0_99:
	add.s32 	%r264, %r366, %r8;
	shl.b32 	%r265, %r264, 8;
	mul.wide.s32 	%rd276, %r265, 4;
	add.s64 	%rd444, %rd3, %rd276;
	add.s64 	%rd445, %rd180, %rd276;
	mov.u16 	%rs175, 0;
	bra.uni 	$L__BB0_100;

$L__BB0_31:
	add.s32 	%r187, %r37, %r360;
	setp.ne.s32 	%p32, %r187, %r5;
	setp.lt.s32 	%p33, %r37, 1;
	mov.u64 	%rd443, 0;
	or.pred  	%p34, %p33, %p32;
	@%p34 bra 	$L__BB0_34;

	add.s32 	%r188, %r363, %r8;
	shl.b32 	%r189, %r188, 8;
	mul.wide.s32 	%rd213, %r189, 4;
	add.s64 	%rd442, %rd3, %rd213;
	add.s64 	%rd443, %rd180, %rd213;
	mov.u16 	%rs174, 1;

$L__BB0_34:
	mov.u16 	%rs175, 0;
	mov.u64 	%rd444, 0;
	setp.eq.s32 	%p35, %r11, 1;
	mov.u64 	%rd445, %rd444;
	@%p35 bra 	$L__BB0_122;

	mul.wide.s32 	%rd379, %r363, 4;
	add.s64 	%rd378, %rd1, %rd379;
	ld.global.u32 	%r39, [%rd378+4];
	setp.ne.s64 	%p36, %rd443, 0;
	@%p36 bra 	$L__BB0_40;

	add.s32 	%r192, %r39, %r5;
	setp.eq.s32 	%p37, %r192, %r360;
	@%p37 bra 	$L__BB0_39;
	bra.uni 	$L__BB0_37;

$L__BB0_39:
	add.s32 	%r336, %r363, 1;
	add.s32 	%r196, %r336, %r8;
	shl.b32 	%r197, %r196, 8;
	mul.wide.s32 	%rd219, %r197, 4;
	add.s64 	%rd442, %rd3, %rd219;
	add.s64 	%rd443, %rd180, %rd219;
	mov.u16 	%rs174, 0;
	bra.uni 	$L__BB0_40;

$L__BB0_97:
	add.s32 	%r261, %r54, %r360;
	setp.ne.s32 	%p97, %r261, %r7;
	setp.lt.s32 	%p98, %r54, 1;
	mov.u64 	%rd445, 0;
	or.pred  	%p99, %p98, %p97;
	@%p99 bra 	$L__BB0_100;

	add.s32 	%r262, %r366, %r8;
	shl.b32 	%r263, %r262, 8;
	mul.wide.s32 	%rd275, %r263, 4;
	add.s64 	%rd444, %rd3, %rd275;
	add.s64 	%rd445, %rd180, %rd275;
	mov.u16 	%rs175, 1;

$L__BB0_100:
	setp.eq.s32 	%p100, %r11, 1;
	@%p100 bra 	$L__BB0_122;

	mul.wide.s32 	%rd383, %r366, 4;
	add.s64 	%rd382, %rd1, %rd383;
	ld.global.u32 	%r56, [%rd382+4];
	setp.ne.s64 	%p101, %rd443, 0;
	@%p101 bra 	$L__BB0_106;

	add.s32 	%r266, %r56, %r5;
	setp.eq.s32 	%p102, %r266, %r360;
	@%p102 bra 	$L__BB0_105;
	bra.uni 	$L__BB0_103;

$L__BB0_105:
	add.s32 	%r352, %r366, 1;
	add.s32 	%r270, %r352, %r8;
	shl.b32 	%r271, %r270, 8;
	mul.wide.s32 	%rd279, %r271, 4;
	add.s64 	%rd442, %rd3, %rd279;
	add.s64 	%rd443, %rd180, %rd279;
	mov.u16 	%rs174, 0;
	bra.uni 	$L__BB0_106;

$L__BB0_37:
	add.s32 	%r193, %r39, %r360;
	setp.ne.s32 	%p38, %r193, %r5;
	setp.lt.s32 	%p39, %r39, 1;
	mov.u64 	%rd443, 0;
	or.pred  	%p40, %p39, %p38;
	@%p40 bra 	$L__BB0_40;

	add.s32 	%r335, %r363, 1;
	add.s32 	%r194, %r335, %r8;
	shl.b32 	%r195, %r194, 8;
	mul.wide.s32 	%rd218, %r195, 4;
	add.s64 	%rd442, %rd3, %rd218;
	add.s64 	%rd443, %rd180, %rd218;
	mov.u16 	%rs174, 1;

$L__BB0_40:
	mov.u16 	%rs175, 0;
	mov.u64 	%rd444, 0;
	add.s32 	%r40, %r363, 2;
	setp.eq.s32 	%p41, %r11, 2;
	mov.u64 	%rd445, %rd444;
	@%p41 bra 	$L__BB0_122;

	mul.wide.s32 	%rd381, %r363, 4;
	add.s64 	%rd380, %rd1, %rd381;
	mov.u16 	%rs175, 0;
	mov.u64 	%rd444, 0;
	ld.global.u32 	%r41, [%rd380+8];
	setp.ne.s64 	%p42, %rd443, 0;
	@%p42 bra 	$L__BB0_122;

	add.s32 	%r198, %r41, %r5;
	setp.eq.s32 	%p43, %r198, %r360;
	@%p43 bra 	$L__BB0_45;
	bra.uni 	$L__BB0_43;

$L__BB0_45:
	mov.u64 	%rd444, 0;
	add.s32 	%r202, %r40, %r8;
	shl.b32 	%r203, %r202, 8;
	mul.wide.s32 	%rd232, %r203, 4;
	add.s64 	%rd442, %rd3, %rd232;
	add.s64 	%rd443, %rd180, %rd232;
	mov.u16 	%rs174, 0;
	mov.u64 	%rd445, %rd444;
	mov.u16 	%rs175, %rs174;
	bra.uni 	$L__BB0_122;

$L__BB0_103:
	add.s32 	%r267, %r56, %r360;
	setp.ne.s32 	%p103, %r267, %r5;
	setp.lt.s32 	%p104, %r56, 1;
	mov.u64 	%rd443, 0;
	or.pred  	%p105, %p104, %p103;
	@%p105 bra 	$L__BB0_106;

	add.s32 	%r351, %r366, 1;
	add.s32 	%r268, %r351, %r8;
	shl.b32 	%r269, %r268, 8;
	mul.wide.s32 	%rd278, %r269, 4;
	add.s64 	%rd442, %rd3, %rd278;
	add.s64 	%rd443, %rd180, %rd278;
	mov.u16 	%rs174, 1;

$L__BB0_106:
	setp.ne.s64 	%p106, %rd445, 0;
	@%p106 bra 	$L__BB0_111;

	add.s32 	%r272, %r56, %r7;
	setp.eq.s32 	%p107, %r272, %r360;
	@%p107 bra 	$L__BB0_110;
	bra.uni 	$L__BB0_108;

$L__BB0_110:
	add.s32 	%r350, %r366, 1;
	add.s32 	%r276, %r350, %r8;
	shl.b32 	%r277, %r276, 8;
	mul.wide.s32 	%rd282, %r277, 4;
	add.s64 	%rd444, %rd3, %rd282;
	add.s64 	%rd445, %rd180, %rd282;
	mov.u16 	%rs175, 0;
	bra.uni 	$L__BB0_111;

$L__BB0_108:
	add.s32 	%r273, %r56, %r360;
	setp.ne.s32 	%p108, %r273, %r7;
	setp.lt.s32 	%p109, %r56, 1;
	mov.u64 	%rd445, 0;
	or.pred  	%p110, %p109, %p108;
	@%p110 bra 	$L__BB0_111;

	add.s32 	%r349, %r366, 1;
	add.s32 	%r274, %r349, %r8;
	shl.b32 	%r275, %r274, 8;
	mul.wide.s32 	%rd281, %r275, 4;
	add.s64 	%rd444, %rd3, %rd281;
	add.s64 	%rd445, %rd180, %rd281;
	mov.u16 	%rs175, 1;

$L__BB0_111:
	setp.eq.s32 	%p111, %r11, 2;
	@%p111 bra 	$L__BB0_122;

	mul.wide.s32 	%rd385, %r366, 4;
	add.s64 	%rd384, %rd1, %rd385;
	ld.global.u32 	%r58, [%rd384+8];
	setp.ne.s64 	%p112, %rd443, 0;
	@%p112 bra 	$L__BB0_117;

	add.s32 	%r278, %r58, %r5;
	setp.eq.s32 	%p113, %r278, %r360;
	@%p113 bra 	$L__BB0_116;
	bra.uni 	$L__BB0_114;

$L__BB0_116:
	add.s32 	%r359, %r366, 2;
	add.s32 	%r282, %r359, %r8;
	shl.b32 	%r283, %r282, 8;
	mul.wide.s32 	%rd285, %r283, 4;
	add.s64 	%rd442, %rd3, %rd285;
	add.s64 	%rd443, %rd180, %rd285;
	mov.u16 	%rs174, 0;
	bra.uni 	$L__BB0_117;

$L__BB0_43:
	mov.u16 	%rs175, 0;
	add.s32 	%r199, %r41, %r360;
	setp.ne.s32 	%p44, %r199, %r5;
	setp.lt.s32 	%p45, %r41, 1;
	mov.u64 	%rd444, 0;
	or.pred  	%p46, %p45, %p44;
	mov.u64 	%rd443, %rd444;
	mov.u64 	%rd445, %rd444;
	@%p46 bra 	$L__BB0_122;

	mov.u16 	%rs175, 0;
	add.s32 	%r200, %r40, %r8;
	shl.b32 	%r201, %r200, 8;
	mul.wide.s32 	%rd229, %r201, 4;
	add.s64 	%rd442, %rd3, %rd229;
	add.s64 	%rd443, %rd180, %rd229;
	mov.u16 	%rs174, 1;
	bra.uni 	$L__BB0_122;

$L__BB0_114:
	add.s32 	%r279, %r58, %r360;
	setp.ne.s32 	%p114, %r279, %r5;
	setp.lt.s32 	%p115, %r58, 1;
	mov.u64 	%rd443, 0;
	or.pred  	%p116, %p115, %p114;
	@%p116 bra 	$L__BB0_117;

	add.s32 	%r358, %r366, 2;
	add.s32 	%r280, %r358, %r8;
	shl.b32 	%r281, %r280, 8;
	mul.wide.s32 	%rd284, %r281, 4;
	add.s64 	%rd442, %rd3, %rd284;
	add.s64 	%rd443, %rd180, %rd284;
	mov.u16 	%rs174, 1;

$L__BB0_117:
	setp.ne.s64 	%p117, %rd445, 0;
	@%p117 bra 	$L__BB0_122;

	add.s32 	%r284, %r58, %r7;
	setp.eq.s32 	%p118, %r284, %r360;
	@%p118 bra 	$L__BB0_121;
	bra.uni 	$L__BB0_119;

$L__BB0_121:
	add.s32 	%r357, %r366, 2;
	add.s32 	%r288, %r357, %r8;
	shl.b32 	%r289, %r288, 8;
	mul.wide.s32 	%rd288, %r289, 4;
	add.s64 	%rd444, %rd3, %rd288;
	add.s64 	%rd445, %rd180, %rd288;
	mov.u16 	%rs175, 0;
	bra.uni 	$L__BB0_122;

$L__BB0_119:
	add.s32 	%r285, %r58, %r360;
	setp.ne.s32 	%p119, %r285, %r7;
	setp.lt.s32 	%p120, %r58, 1;
	mov.u64 	%rd445, 0;
	or.pred  	%p121, %p120, %p119;
	@%p121 bra 	$L__BB0_122;

	add.s32 	%r356, %r366, 2;
	add.s32 	%r286, %r356, %r8;
	shl.b32 	%r287, %r286, 8;
	mul.wide.s32 	%rd287, %r287, 4;
	add.s64 	%rd444, %rd3, %rd287;
	add.s64 	%rd445, %rd180, %rd287;
	mov.u16 	%rs175, 1;

$L__BB0_122:
	or.b64  	%rd289, %rd443, %rd445;
	setp.eq.s64 	%p122, %rd289, 0;
	@%p122 bra 	$L__BB0_204;

	shr.u32 	%r321, %r2, 3;
	shl.b32 	%r59, %r360, 4;
	or.b32  	%r60, %r321, %r59;
	setp.ge.s32 	%p123, %r60, %r71;
	mov.f32 	%f143, 0f00000000;
	mov.f32 	%f142, %f143;
	@%p123 bra 	$L__BB0_125;

	add.s32 	%r290, %r13, %r60;
	mul.wide.s32 	%rd290, %r290, 4;
	add.s64 	%rd291, %rd2, %rd290;
	ld.global.f32 	%f142, [%rd291];

$L__BB0_125:
	shl.b32 	%r353, %r360, 4;
	or.b32  	%r323, %r2, 32;
	shr.u32 	%r322, %r323, 3;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs90, %f142;}

	// end inline asm
	st.shared.u16 	[%r14], %rs90;
	add.s32 	%r61, %r322, %r353;
	setp.ge.s32 	%p124, %r61, %r71;
	@%p124 bra 	$L__BB0_127;

	add.s32 	%r291, %r13, %r61;
	mul.wide.s32 	%rd292, %r291, 4;
	add.s64 	%rd293, %rd2, %rd292;
	ld.global.f32 	%f143, [%rd293];

$L__BB0_127:
	shl.b32 	%r354, %r360, 4;
	or.b32  	%r325, %r2, 64;
	shr.u32 	%r324, %r325, 3;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs91, %f143;}

	// end inline asm
	st.shared.u16 	[%r14+64], %rs91;
	add.s32 	%r62, %r324, %r354;
	setp.ge.s32 	%p125, %r62, %r71;
	mov.f32 	%f145, 0f00000000;
	mov.f32 	%f144, %f145;
	@%p125 bra 	$L__BB0_129;

	add.s32 	%r292, %r13, %r62;
	mul.wide.s32 	%rd294, %r292, 4;
	add.s64 	%rd295, %rd2, %rd294;
	ld.global.f32 	%f144, [%rd295];

$L__BB0_129:
	shl.b32 	%r355, %r360, 4;
	or.b32  	%r327, %r2, 96;
	shr.u32 	%r326, %r327, 3;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs92, %f144;}

	// end inline asm
	st.shared.u16 	[%r14+128], %rs92;
	add.s32 	%r63, %r326, %r355;
	setp.ge.s32 	%p126, %r63, %r71;
	@%p126 bra 	$L__BB0_131;

	add.s32 	%r293, %r13, %r63;
	mul.wide.s32 	%rd296, %r293, 4;
	add.s64 	%rd297, %rd2, %rd296;
	ld.global.f32 	%f145, [%rd297];

$L__BB0_131:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs93, %f145;}

	// end inline asm
	st.shared.u16 	[%r14+192], %rs93;
	bar.warp.sync 	-1;
	setp.eq.s64 	%p127, %rd443, 0;
	@%p127 bra 	$L__BB0_137;

	and.b16  	%rs94, %rs174, 255;
	setp.ne.s16 	%p128, %rs94, 0;
	mov.u32 	%r367, %r20;
	@%p128 bra 	$L__BB0_134;

	mov.u32 	%r367, %r2;

$L__BB0_134:
	mul.wide.u32 	%rd298, %r367, 4;
	add.s64 	%rd299, %rd442, %rd298;
	ld.global.f32 	%f59, [%rd299];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs95, %f59;}

	// end inline asm
	st.shared.u16 	[%r18], %rs95;
	setp.eq.s16 	%p129, %rs94, 0;
	@%p129 bra 	$L__BB0_136;

	add.s64 	%rd301, %rd442, %rd300;
	ld.global.f32 	%f146, [%rd301];
	bra.uni 	$L__BB0_138;

$L__BB0_137:
	mov.f32 	%f146, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs97, %f146;}

	// end inline asm
	st.shared.u16 	[%r18], %rs97;
	bra.uni 	$L__BB0_138;

$L__BB0_136:
	add.s64 	%rd303, %rd442, %rd302;
	ld.global.f32 	%f146, [%rd303];

$L__BB0_138:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs98, %f146;}

	// end inline asm
	st.shared.u16 	[%r18+64], %rs98;
	@%p127 bra 	$L__BB0_147;

	and.b16  	%rs99, %rs174, 255;
	setp.eq.s16 	%p131, %rs99, 0;
	@%p131 bra 	$L__BB0_141;

	add.s64 	%rd305, %rd442, %rd304;
	ld.global.f32 	%f147, [%rd305];
	bra.uni 	$L__BB0_142;

$L__BB0_147:
	mov.f32 	%f148, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs103, %f148;}

	// end inline asm
	st.shared.u16 	[%r18+128], %rs103;
	bra.uni 	$L__BB0_148;

$L__BB0_141:
	add.s64 	%rd307, %rd442, %rd306;
	ld.global.f32 	%f147, [%rd307];

$L__BB0_142:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs100, %f147;}

	// end inline asm
	st.shared.u16 	[%r18+128], %rs100;
	@%p127 bra 	$L__BB0_146;

	and.b16  	%rs144, %rs174, 255;
	setp.eq.s16 	%p159, %rs144, 0;
	@%p159 bra 	$L__BB0_145;

	add.s64 	%rd309, %rd442, %rd308;
	ld.global.f32 	%f148, [%rd309];
	bra.uni 	$L__BB0_148;

$L__BB0_146:
	mov.f32 	%f149, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs102, %f149;}

	// end inline asm
	st.shared.u16 	[%r18+192], %rs102;
	bra.uni 	$L__BB0_152;

$L__BB0_145:
	add.s64 	%rd311, %rd442, %rd310;
	ld.global.f32 	%f148, [%rd311];

$L__BB0_148:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs104, %f148;}

	// end inline asm
	st.shared.u16 	[%r18+192], %rs104;
	@%p127 bra 	$L__BB0_157;

	and.b16  	%rs105, %rs174, 255;
	setp.eq.s16 	%p135, %rs105, 0;
	@%p135 bra 	$L__BB0_151;

	add.s64 	%rd313, %rd442, %rd312;
	ld.global.f32 	%f149, [%rd313];
	bra.uni 	$L__BB0_152;

$L__BB0_157:
	mov.f32 	%f150, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs109, %f150;}

	// end inline asm
	st.shared.u16 	[%r18+256], %rs109;
	bra.uni 	$L__BB0_158;

$L__BB0_151:
	add.s64 	%rd315, %rd442, %rd314;
	ld.global.f32 	%f149, [%rd315];

$L__BB0_152:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs106, %f149;}

	// end inline asm
	st.shared.u16 	[%r18+256], %rs106;
	@%p127 bra 	$L__BB0_156;

	and.b16  	%rs107, %rs174, 255;
	setp.eq.s16 	%p137, %rs107, 0;
	@%p137 bra 	$L__BB0_155;

	add.s64 	%rd317, %rd442, %rd316;
	ld.global.f32 	%f150, [%rd317];
	bra.uni 	$L__BB0_158;

$L__BB0_156:
	mov.f32 	%f151, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs108, %f151;}

	// end inline asm
	st.shared.u16 	[%r18+320], %rs108;
	bra.uni 	$L__BB0_162;

$L__BB0_155:
	add.s64 	%rd319, %rd442, %rd318;
	ld.global.f32 	%f150, [%rd319];

$L__BB0_158:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs110, %f150;}

	// end inline asm
	st.shared.u16 	[%r18+320], %rs110;
	@%p127 bra 	$L__BB0_166;

	and.b16  	%rs111, %rs174, 255;
	setp.eq.s16 	%p139, %rs111, 0;
	@%p139 bra 	$L__BB0_161;

	add.s64 	%rd321, %rd442, %rd320;
	ld.global.f32 	%f151, [%rd321];
	bra.uni 	$L__BB0_162;

$L__BB0_166:
	mov.f32 	%f152, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs114, %f152;}

	// end inline asm
	st.shared.u16 	[%r18+384], %rs114;
	bra.uni 	$L__BB0_167;

$L__BB0_161:
	add.s64 	%rd323, %rd442, %rd322;
	ld.global.f32 	%f151, [%rd323];

$L__BB0_162:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs112, %f151;}

	// end inline asm
	st.shared.u16 	[%r18+384], %rs112;
	mov.f32 	%f152, 0f00000000;
	@%p127 bra 	$L__BB0_167;

	and.b16  	%rs113, %rs174, 255;
	setp.eq.s16 	%p141, %rs113, 0;
	@%p141 bra 	$L__BB0_165;

	add.s64 	%rd325, %rd442, %rd324;
	ld.global.f32 	%f152, [%rd325];
	bra.uni 	$L__BB0_167;

$L__BB0_165:
	add.s64 	%rd327, %rd442, %rd326;
	ld.global.f32 	%f152, [%rd327];

$L__BB0_167:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs115, %f152;}

	// end inline asm
	st.shared.u16 	[%r18+448], %rs115;
	setp.eq.s64 	%p142, %rd445, 0;
	@%p142 bra 	$L__BB0_173;

	and.b16  	%rs116, %rs175, 255;
	setp.ne.s16 	%p143, %rs116, 0;
	mov.u32 	%r368, %r22;
	@%p143 bra 	$L__BB0_170;

	mov.u32 	%r368, %r21;

$L__BB0_170:
	mul.wide.s32 	%rd328, %r368, 4;
	add.s64 	%rd329, %rd444, %rd328;
	ld.global.f32 	%f80, [%rd329];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs117, %f80;}

	// end inline asm
	st.shared.u16 	[%r18+512], %rs117;
	setp.eq.s16 	%p144, %rs116, 0;
	@%p144 bra 	$L__BB0_172;

	add.s64 	%rd331, %rd444, %rd330;
	ld.global.f32 	%f153, [%rd331];
	bra.uni 	$L__BB0_174;

$L__BB0_173:
	mov.f32 	%f153, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs119, %f153;}

	// end inline asm
	st.shared.u16 	[%r18+512], %rs119;
	bra.uni 	$L__BB0_174;

$L__BB0_172:
	add.s64 	%rd333, %rd444, %rd332;
	ld.global.f32 	%f153, [%rd333];

$L__BB0_174:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs120, %f153;}

	// end inline asm
	st.shared.u16 	[%r18+576], %rs120;
	@%p142 bra 	$L__BB0_183;

	and.b16  	%rs121, %rs175, 255;
	setp.eq.s16 	%p146, %rs121, 0;
	@%p146 bra 	$L__BB0_177;

	add.s64 	%rd335, %rd444, %rd334;
	ld.global.f32 	%f154, [%rd335];
	bra.uni 	$L__BB0_178;

$L__BB0_183:
	mov.f32 	%f155, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs125, %f155;}

	// end inline asm
	st.shared.u16 	[%r18+640], %rs125;
	bra.uni 	$L__BB0_184;

$L__BB0_177:
	add.s64 	%rd337, %rd444, %rd336;
	ld.global.f32 	%f154, [%rd337];

$L__BB0_178:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs122, %f154;}

	// end inline asm
	st.shared.u16 	[%r18+640], %rs122;
	@%p142 bra 	$L__BB0_182;

	and.b16  	%rs145, %rs175, 255;
	setp.eq.s16 	%p160, %rs145, 0;
	@%p160 bra 	$L__BB0_181;

	add.s64 	%rd339, %rd444, %rd338;
	ld.global.f32 	%f155, [%rd339];
	bra.uni 	$L__BB0_184;

$L__BB0_182:
	mov.f32 	%f156, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs124, %f156;}

	// end inline asm
	st.shared.u16 	[%r18+704], %rs124;
	bra.uni 	$L__BB0_188;

$L__BB0_181:
	add.s64 	%rd341, %rd444, %rd340;
	ld.global.f32 	%f155, [%rd341];

$L__BB0_184:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs126, %f155;}

	// end inline asm
	st.shared.u16 	[%r18+704], %rs126;
	@%p142 bra 	$L__BB0_193;

	and.b16  	%rs127, %rs175, 255;
	setp.eq.s16 	%p150, %rs127, 0;
	@%p150 bra 	$L__BB0_187;

	add.s64 	%rd343, %rd444, %rd342;
	ld.global.f32 	%f156, [%rd343];
	bra.uni 	$L__BB0_188;

$L__BB0_193:
	mov.f32 	%f157, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs131, %f157;}

	// end inline asm
	st.shared.u16 	[%r18+768], %rs131;
	bra.uni 	$L__BB0_194;

$L__BB0_187:
	add.s64 	%rd345, %rd444, %rd344;
	ld.global.f32 	%f156, [%rd345];

$L__BB0_188:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs128, %f156;}

	// end inline asm
	st.shared.u16 	[%r18+768], %rs128;
	@%p142 bra 	$L__BB0_192;

	and.b16  	%rs129, %rs175, 255;
	setp.eq.s16 	%p152, %rs129, 0;
	@%p152 bra 	$L__BB0_191;

	add.s64 	%rd347, %rd444, %rd346;
	ld.global.f32 	%f157, [%rd347];
	bra.uni 	$L__BB0_194;

$L__BB0_192:
	mov.f32 	%f158, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs130, %f158;}

	// end inline asm
	st.shared.u16 	[%r18+832], %rs130;
	bra.uni 	$L__BB0_198;

$L__BB0_191:
	add.s64 	%rd349, %rd444, %rd348;
	ld.global.f32 	%f157, [%rd349];

$L__BB0_194:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs132, %f157;}

	// end inline asm
	st.shared.u16 	[%r18+832], %rs132;
	@%p142 bra 	$L__BB0_202;

	and.b16  	%rs133, %rs175, 255;
	setp.eq.s16 	%p154, %rs133, 0;
	@%p154 bra 	$L__BB0_197;

	add.s64 	%rd351, %rd444, %rd350;
	ld.global.f32 	%f158, [%rd351];
	bra.uni 	$L__BB0_198;

$L__BB0_202:
	mov.f32 	%f159, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs136, %f159;}

	// end inline asm
	st.shared.u16 	[%r18+896], %rs136;
	bra.uni 	$L__BB0_203;

$L__BB0_197:
	add.s64 	%rd353, %rd444, %rd352;
	ld.global.f32 	%f158, [%rd353];

$L__BB0_198:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs134, %f158;}

	// end inline asm
	st.shared.u16 	[%r18+896], %rs134;
	mov.f32 	%f159, 0f00000000;
	@%p142 bra 	$L__BB0_203;

	and.b16  	%rs135, %rs175, 255;
	setp.eq.s16 	%p156, %rs135, 0;
	@%p156 bra 	$L__BB0_201;

	add.s64 	%rd355, %rd444, %rd354;
	ld.global.f32 	%f159, [%rd355];
	bra.uni 	$L__BB0_203;

$L__BB0_201:
	add.s64 	%rd357, %rd444, %rd356;
	ld.global.f32 	%f159, [%rd357];

$L__BB0_203:
	mov.u32 	%r330, _ZZ41batch8_symmetric_block_pair_multiply_wmmaE6temp_c;
	mov.u32 	%r329, _ZZ41batch8_symmetric_block_pair_multiply_wmmaE8shared_b;
	mov.u32 	%r328, _ZZ41batch8_symmetric_block_pair_multiply_wmmaE8shared_a;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs137, %f159;}

	// end inline asm
	st.shared.u16 	[%r18+960], %rs137;
	bar.warp.sync 	-1;
	mov.u32 	%r294, 16;
	wmma.load.a.sync.aligned.row.m32n8k16.shared.f16 	{%r296, %r297, %r298, %r299, %r300, %r301, %r302, %r303}, [%r328], %r294;
	mov.u32 	%r304, 8;
	wmma.load.b.sync.aligned.row.m32n8k16.shared.f16 	{%r306, %r307, %r308, %r309, %r310, %r311, %r312, %r313}, [%r329], %r304;
	mov.f32 	%f101, 0f00000000;
	wmma.mma.sync.aligned.row.row.m32n8k16.f32.f32 {%f102, %f103, %f104, %f105, %f106, %f107, %f108, %f109}, {%r296, %r297, %r298, %r299, %r300, %r301, %r302, %r303}, {%r306, %r307, %r308, %r309, %r310, %r311, %r312, %r313}, {%f101, %f101, %f101, %f101, %f101, %f101, %f101, %f101};
	wmma.store.d.sync.aligned.row.m32n8k16.shared.f32 	[%r330], {%f102, %f103, %f104, %f105, %f106, %f107, %f108, %f109}, %r304;
	bar.warp.sync 	-1;
	ld.shared.f32 	%f110, [%r6];
	ld.shared.f32 	%f111, [%r23];
	add.f32 	%f112, %f111, %f110;
	st.shared.f32 	[%r6], %f112;
	ld.shared.f32 	%f113, [%r6+4];
	ld.shared.f32 	%f114, [%r23+4];
	add.f32 	%f115, %f114, %f113;
	st.shared.f32 	[%r6+4], %f115;
	ld.shared.f32 	%f116, [%r6+8];
	ld.shared.f32 	%f117, [%r23+8];
	add.f32 	%f118, %f117, %f116;
	st.shared.f32 	[%r6+8], %f118;
	ld.shared.f32 	%f119, [%r6+12];
	ld.shared.f32 	%f120, [%r23+12];
	add.f32 	%f121, %f120, %f119;
	st.shared.f32 	[%r6+12], %f121;
	ld.shared.f32 	%f122, [%r6+16];
	ld.shared.f32 	%f123, [%r23+16];
	add.f32 	%f124, %f123, %f122;
	st.shared.f32 	[%r6+16], %f124;
	ld.shared.f32 	%f125, [%r6+20];
	ld.shared.f32 	%f126, [%r23+20];
	add.f32 	%f127, %f126, %f125;
	st.shared.f32 	[%r6+20], %f127;
	ld.shared.f32 	%f128, [%r6+24];
	ld.shared.f32 	%f129, [%r23+24];
	add.f32 	%f130, %f129, %f128;
	st.shared.f32 	[%r6+24], %f130;
	ld.shared.f32 	%f131, [%r6+28];
	ld.shared.f32 	%f132, [%r23+28];
	add.f32 	%f133, %f132, %f131;
	st.shared.f32 	[%r6+28], %f133;
	bar.warp.sync 	-1;

$L__BB0_204:
	add.s32 	%r360, %r360, 1;
	setp.lt.s32 	%p157, %r360, %r69;
	@%p157 bra 	$L__BB0_3;

$L__BB0_205:
	mov.u32 	%r318, %ctaid.y;
	shl.b32 	%r315, %r318, 5;
	or.b32  	%r67, %r315, %r2;
	setp.ge.s32 	%p158, %r67, %r71;
	@%p158 bra 	$L__BB0_207;

	ld.param.u64 	%rd372, [batch8_symmetric_block_pair_multiply_wmma_param_3];
	ld.param.u32 	%r320, [batch8_symmetric_block_pair_multiply_wmma_param_5];
	mov.u32 	%r319, %ctaid.x;
	ld.shared.f32 	%f134, [%r6];
	mad.lo.s32 	%r316, %r319, %r71, %r67;
	cvta.to.global.u64 	%rd361, %rd372;
	mul.wide.s32 	%rd362, %r316, 4;
	add.s64 	%rd363, %rd361, %rd362;
	st.global.f32 	[%rd363], %f134;
	ld.shared.f32 	%f135, [%r6+4];
	mul.lo.s32 	%r317, %r71, %r320;
	mul.wide.s32 	%rd364, %r317, 4;
	add.s64 	%rd365, %rd363, %rd364;
	st.global.f32 	[%rd365], %f135;
	ld.shared.f32 	%f136, [%r6+8];
	add.s64 	%rd366, %rd365, %rd364;
	st.global.f32 	[%rd366], %f136;
	ld.shared.f32 	%f137, [%r6+12];
	add.s64 	%rd367, %rd366, %rd364;
	st.global.f32 	[%rd367], %f137;
	ld.shared.f32 	%f138, [%r6+16];
	add.s64 	%rd368, %rd367, %rd364;
	st.global.f32 	[%rd368], %f138;
	ld.shared.f32 	%f139, [%r6+20];
	add.s64 	%rd369, %rd368, %rd364;
	st.global.f32 	[%rd369], %f139;
	ld.shared.f32 	%f140, [%r6+24];
	add.s64 	%rd370, %rd369, %rd364;
	st.global.f32 	[%rd370], %f140;
	ld.shared.f32 	%f141, [%r6+28];
	add.s64 	%rd371, %rd370, %rd364;
	st.global.f32 	[%rd371], %f141;

$L__BB0_207:
	ret;

}
	// .globl	batch8_symmetric_block_pair_multiply_wmma_optimized
.visible .entry batch8_symmetric_block_pair_multiply_wmma_optimized(
	.param .u64 batch8_symmetric_block_pair_multiply_wmma_optimized_param_0,
	.param .u64 batch8_symmetric_block_pair_multiply_wmma_optimized_param_1,
	.param .u64 batch8_symmetric_block_pair_multiply_wmma_optimized_param_2,
	.param .u64 batch8_symmetric_block_pair_multiply_wmma_optimized_param_3,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_optimized_param_4,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_optimized_param_5,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_optimized_param_6,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_optimized_param_7,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_optimized_param_8
)
{



	ret;

}

