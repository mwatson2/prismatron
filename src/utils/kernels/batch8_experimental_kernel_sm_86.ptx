//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-34714021
// Cuda compilation tools, release 12.6, V12.6.68
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_86
.address_size 64

	// .globl	batch8_symmetric_block_pair_multiply_wmma_experimental
// _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE8shared_a has been demoted
// _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE8shared_b has been demoted
// _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE13block_results has been demoted
// _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE6temp_c has been demoted

.visible .entry batch8_symmetric_block_pair_multiply_wmma_experimental(
	.param .u64 batch8_symmetric_block_pair_multiply_wmma_experimental_param_0,
	.param .u64 batch8_symmetric_block_pair_multiply_wmma_experimental_param_1,
	.param .u64 batch8_symmetric_block_pair_multiply_wmma_experimental_param_2,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_experimental_param_3,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_experimental_param_4,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_experimental_param_5,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_experimental_param_6,
	.param .u32 batch8_symmetric_block_pair_multiply_wmma_experimental_param_7
)
{
	.reg .pred 	%p<70>;
	.reg .b16 	%rs<97>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<226>;
	.reg .b64 	%rd<214>;
	// demoted variable
	.shared .align 2 .b8 _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE8shared_a[1024];
	// demoted variable
	.shared .align 2 .b8 _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE8shared_b[256];
	// demoted variable
	.shared .align 4 .b8 _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE13block_results[1024];
	// demoted variable
	.shared .align 4 .b8 _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE6temp_c[1024];

	ld.param.u64 	%rd52, [batch8_symmetric_block_pair_multiply_wmma_experimental_param_0];
	ld.param.u64 	%rd54, [batch8_symmetric_block_pair_multiply_wmma_experimental_param_1];
	ld.param.u32 	%r51, [batch8_symmetric_block_pair_multiply_wmma_experimental_param_4];
	ld.param.u32 	%r52, [batch8_symmetric_block_pair_multiply_wmma_experimental_param_5];
	ld.param.u32 	%r53, [batch8_symmetric_block_pair_multiply_wmma_experimental_param_6];
	ld.param.u32 	%r54, [batch8_symmetric_block_pair_multiply_wmma_experimental_param_7];
	cvta.to.global.u64 	%rd1, %rd54;
	cvta.to.global.u64 	%rd2, %rd52;
	mov.u32 	%r1, %tid.x;
	and.b32  	%r2, %r1, 31;
	mov.u32 	%r3, %ctaid.x;
	setp.ge.s32 	%p1, %r3, %r51;
	mov.u32 	%r4, %ctaid.y;
	shl.b32 	%r5, %r4, 1;
	setp.ge.s32 	%p2, %r5, %r52;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_120;

	shl.b32 	%r55, %r2, 5;
	mov.u32 	%r56, _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE13block_results;
	add.s32 	%r6, %r56, %r55;
	mov.u32 	%r223, 0;
	st.shared.u32 	[%r6], %r223;
	st.shared.u32 	[%r6+4], %r223;
	st.shared.u32 	[%r6+8], %r223;
	st.shared.u32 	[%r6+12], %r223;
	st.shared.u32 	[%r6+16], %r223;
	st.shared.u32 	[%r6+20], %r223;
	st.shared.u32 	[%r6+24], %r223;
	st.shared.u32 	[%r6+28], %r223;
	add.s32 	%r7, %r5, 1;
	bar.warp.sync 	-1;
	setp.lt.s32 	%p4, %r52, 1;
	@%p4 bra 	$L__BB0_118;

	shl.b32 	%r8, %r52, 8;
	shr.u32 	%r214, %r2, 3;
	and.b32  	%r59, %r1, 7;
	mad.lo.s32 	%r60, %r59, %r51, %r3;
	mul.lo.s32 	%r10, %r60, %r54;
	shl.b32 	%r61, %r2, 1;
	mov.u32 	%r62, _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE8shared_b;
	add.s32 	%r11, %r62, %r61;
	or.b32  	%r63, %r2, 32;
	shr.u32 	%r215, %r63, 3;
	or.b32  	%r64, %r2, 64;
	shr.u32 	%r216, %r64, 3;
	or.b32  	%r65, %r2, 96;
	shr.u32 	%r217, %r65, 3;
	mov.u32 	%r67, _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE6temp_c;
	add.s32 	%r15, %r67, %r55;
	mov.u32 	%r68, _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE8shared_a;
	add.s32 	%r16, %r68, %r61;
	and.b32  	%r69, %r1, 15;
	shr.u32 	%r70, %r63, 4;
	shl.b32 	%r71, %r1, 4;
	and.b32  	%r72, %r71, 240;
	shr.u32 	%r73, %r2, 4;
	or.b32  	%r18, %r72, %r73;
	shr.u32 	%r74, %r64, 4;
	and.b32  	%r75, %r63, 48;
	or.b32  	%r76, %r75, %r69;
	or.b32  	%r77, %r72, %r70;
	shr.u32 	%r78, %r65, 4;
	and.b32  	%r79, %r64, 80;
	or.b32  	%r80, %r79, %r69;
	or.b32  	%r81, %r72, %r74;
	or.b32  	%r82, %r2, 128;
	shr.u32 	%r83, %r82, 4;
	cvt.u64.u32 	%rd3, %r76;
	cvt.u64.u32 	%rd4, %r77;
	and.b32  	%r84, %r65, 112;
	or.b32  	%r85, %r84, %r69;
	or.b32  	%r86, %r72, %r78;
	or.b32  	%r87, %r2, 160;
	shr.u32 	%r88, %r87, 4;
	cvt.u64.u32 	%rd5, %r80;
	cvt.u64.u32 	%rd6, %r81;
	and.b32  	%r89, %r82, 144;
	or.b32  	%r90, %r89, %r69;
	or.b32  	%r91, %r72, %r83;
	or.b32  	%r92, %r2, 192;
	shr.u32 	%r93, %r92, 4;
	cvt.u64.u32 	%rd7, %r85;
	cvt.u64.u32 	%rd8, %r86;
	and.b32  	%r94, %r87, 176;
	or.b32  	%r95, %r94, %r69;
	or.b32  	%r96, %r72, %r88;
	or.b32  	%r97, %r2, 224;
	shr.u32 	%r98, %r97, 4;
	cvt.u64.u32 	%rd9, %r90;
	cvt.u64.u32 	%rd10, %r91;
	and.b32  	%r99, %r92, 208;
	or.b32  	%r100, %r99, %r69;
	or.b32  	%r101, %r72, %r93;
	cvt.u64.u32 	%rd11, %r95;
	cvt.u64.u32 	%rd12, %r96;
	or.b32  	%r102, %r2, 256;
	mov.u32 	%r103, 256;
	and.b32  	%r104, %r102, 272;
	or.b32  	%r105, %r104, %r69;
	add.s32 	%r106, %r105, -256;
	cvt.s64.s32 	%rd13, %r106;
	shr.u32 	%r107, %r102, 4;
	add.s32 	%r108, %r72, %r107;
	add.s32 	%r109, %r108, -16;
	cvt.s64.s32 	%rd14, %r109;
	and.b32  	%r110, %r97, 240;
	or.b32  	%r111, %r110, %r69;
	or.b32  	%r112, %r72, %r98;
	cvt.u64.u32 	%rd15, %r100;
	cvt.u64.u32 	%rd16, %r101;
	or.b32  	%r113, %r2, 288;
	and.b32  	%r114, %r113, 304;
	or.b32  	%r115, %r114, %r69;
	add.s32 	%r116, %r115, -256;
	cvt.s64.s32 	%rd17, %r116;
	shr.u32 	%r117, %r113, 4;
	add.s32 	%r118, %r72, %r117;
	add.s32 	%r119, %r118, -16;
	cvt.s64.s32 	%rd18, %r119;
	cvt.u64.u32 	%rd19, %r111;
	cvt.u64.u32 	%rd20, %r112;
	or.b32  	%r120, %r2, 320;
	and.b32  	%r121, %r120, 336;
	or.b32  	%r122, %r121, %r69;
	add.s32 	%r123, %r122, -256;
	cvt.s64.s32 	%rd21, %r123;
	shr.u32 	%r124, %r120, 4;
	add.s32 	%r125, %r72, %r124;
	add.s32 	%r126, %r125, -16;
	cvt.s64.s32 	%rd22, %r126;
	or.b32  	%r127, %r2, 352;
	and.b32  	%r128, %r127, 368;
	or.b32  	%r129, %r128, %r69;
	add.s32 	%r130, %r129, -256;
	cvt.s64.s32 	%rd23, %r130;
	shr.u32 	%r131, %r127, 4;
	add.s32 	%r132, %r72, %r131;
	add.s32 	%r133, %r132, -16;
	cvt.s64.s32 	%rd24, %r133;
	or.b32  	%r134, %r2, 384;
	and.b32  	%r135, %r134, 400;
	or.b32  	%r136, %r135, %r69;
	add.s32 	%r137, %r136, -256;
	cvt.s64.s32 	%rd25, %r137;
	shr.u32 	%r138, %r134, 4;
	add.s32 	%r139, %r72, %r138;
	add.s32 	%r140, %r139, -16;
	cvt.s64.s32 	%rd26, %r140;
	or.b32  	%r141, %r2, 416;
	and.b32  	%r142, %r141, 432;
	or.b32  	%r143, %r142, %r69;
	add.s32 	%r144, %r143, -256;
	cvt.s64.s32 	%rd27, %r144;
	shr.u32 	%r145, %r141, 4;
	add.s32 	%r146, %r72, %r145;
	add.s32 	%r147, %r146, -16;
	cvt.s64.s32 	%rd28, %r147;
	or.b32  	%r148, %r2, 448;
	and.b32  	%r149, %r148, 464;
	or.b32  	%r150, %r149, %r69;
	add.s32 	%r151, %r150, -256;
	cvt.s64.s32 	%rd29, %r151;
	shr.u32 	%r152, %r148, 4;
	add.s32 	%r153, %r72, %r152;
	add.s32 	%r154, %r153, -16;
	cvt.s64.s32 	%rd30, %r154;
	or.b32  	%r155, %r2, 480;
	and.b32  	%r156, %r155, 496;
	or.b32  	%r157, %r156, %r69;
	add.s32 	%r158, %r157, -256;
	cvt.s64.s32 	%rd31, %r158;
	shr.u32 	%r159, %r155, 4;
	add.s32 	%r160, %r72, %r159;
	add.s32 	%r161, %r160, -16;
	cvt.s64.s32 	%rd32, %r161;
	mul.lo.s32 	%r162, %r3, %r53;
	sub.s32 	%r163, %r162, %r5;
	mul.lo.s32 	%r164, %r52, %r163;
	shl.b32 	%r165, %r164, 8;
	shl.b32 	%r166, %r4, 9;
	add.s32 	%r222, %r165, %r166;
	neg.s32 	%r221, %r5;
	add.s32 	%r168, %r162, -1;
	sub.s32 	%r169, %r168, %r5;
	mad.lo.s32 	%r170, %r52, %r169, %r5;
	shl.b32 	%r171, %r170, 8;
	add.s32 	%r220, %r171, 256;
	add.s32 	%r172, %r162, %r5;
	add.s32 	%r173, %r172, 1;
	mul.lo.s32 	%r174, %r52, %r173;
	shl.b32 	%r219, %r174, 8;
	sub.s32 	%r23, %r103, %r8;
	mul.lo.s32 	%r175, %r52, %r172;
	shl.b32 	%r218, %r175, 8;
	shl.b64 	%rd108, %rd14, 2;
	shl.b64 	%rd110, %rd18, 2;
	shl.b64 	%rd112, %rd22, 2;
	shl.b64 	%rd114, %rd24, 2;
	shl.b64 	%rd116, %rd26, 2;
	shl.b64 	%rd118, %rd28, 2;
	shl.b64 	%rd120, %rd30, 2;
	shl.b64 	%rd122, %rd32, 2;
	shl.b64 	%rd154, %rd13, 2;
	shl.b64 	%rd156, %rd17, 2;
	shl.b64 	%rd158, %rd21, 2;
	shl.b64 	%rd160, %rd23, 2;
	shl.b64 	%rd162, %rd25, 2;
	shl.b64 	%rd164, %rd27, 2;
	shl.b64 	%rd166, %rd29, 2;
	shl.b64 	%rd168, %rd31, 2;

$L__BB0_3:
	sub.s32 	%r35, %r7, %r223;
	setp.lt.s32 	%p5, %r221, %r53;
	setp.gt.s32 	%p6, %r221, -1;
	and.pred  	%p7, %p6, %p5;
	@%p7 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_4;

$L__BB0_7:
	mul.wide.s32 	%rd60, %r222, 4;
	add.s64 	%rd37, %rd2, %rd60;
	add.s64 	%rd204, %rd52, %rd60;
	mov.u16 	%rs95, 0;
	bra.uni 	$L__BB0_8;

$L__BB0_4:
	mov.u16 	%rs95, 0;
	mov.u64 	%rd37, 0;
	mov.u64 	%rd204, %rd37;
	@%p6 bra 	$L__BB0_8;

	add.s32 	%r176, %r35, -1;
	setp.lt.s32 	%p9, %r176, 1;
	setp.ge.s32 	%p10, %r176, %r53;
	or.pred  	%p11, %p9, %p10;
	@%p11 bra 	$L__BB0_8;

	mul.wide.s32 	%rd59, %r218, 4;
	add.s64 	%rd37, %rd2, %rd59;
	add.s64 	%rd204, %rd52, %rd59;
	mov.u16 	%rs95, 1;

$L__BB0_8:
	setp.ge.s32 	%p12, %r7, %r52;
	mov.u16 	%rs96, 0;
	mov.u64 	%rd205, 0;
	mov.u64 	%rd206, %rd205;
	@%p12 bra 	$L__BB0_14;

	add.s32 	%r36, %r221, -1;
	setp.gt.s32 	%p13, %r36, -1;
	setp.lt.s32 	%p14, %r36, %r53;
	mov.u16 	%rs96, 0;
	and.pred  	%p15, %p13, %p14;
	@%p15 bra 	$L__BB0_13;
	bra.uni 	$L__BB0_10;

$L__BB0_13:
	mul.wide.s32 	%rd68, %r220, 4;
	add.s64 	%rd205, %rd2, %rd68;
	add.s64 	%rd206, %rd52, %rd68;
	bra.uni 	$L__BB0_14;

$L__BB0_10:
	mov.u64 	%rd205, 0;
	mov.u64 	%rd206, %rd205;
	@%p13 bra 	$L__BB0_14;

	mov.u16 	%rs96, 0;
	mov.u64 	%rd205, 0;
	sub.s32 	%r213, %r7, %r223;
	setp.lt.s32 	%p17, %r213, 1;
	setp.ge.s32 	%p18, %r213, %r53;
	or.pred  	%p19, %p17, %p18;
	@%p19 bra 	$L__BB0_14;

	mul.wide.s32 	%rd67, %r219, 4;
	add.s64 	%rd205, %rd2, %rd67;
	add.s64 	%rd206, %rd52, %rd67;
	mov.u16 	%rs96, 1;

$L__BB0_14:
	or.b64  	%rd69, %rd204, %rd206;
	setp.eq.s64 	%p20, %rd69, 0;
	@%p20 bra 	$L__BB0_117;

	setp.ge.s32 	%p21, %r214, %r54;
	mov.f32 	%f191, 0f00000000;
	mov.f32 	%f190, %f191;
	@%p21 bra 	$L__BB0_17;

	add.s32 	%r177, %r10, %r214;
	mul.wide.s32 	%rd70, %r177, 4;
	add.s64 	%rd71, %rd1, %rd70;
	ld.global.f32 	%f190, [%rd71];

$L__BB0_17:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f190;}

	// end inline asm
	st.shared.u16 	[%r11], %rs12;
	setp.ge.s32 	%p22, %r215, %r54;
	@%p22 bra 	$L__BB0_19;

	add.s32 	%r178, %r10, %r215;
	mul.wide.s32 	%rd72, %r178, 4;
	add.s64 	%rd73, %rd1, %rd72;
	ld.global.f32 	%f191, [%rd73];

$L__BB0_19:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f191;}

	// end inline asm
	st.shared.u16 	[%r11+64], %rs13;
	setp.ge.s32 	%p23, %r216, %r54;
	mov.f32 	%f193, 0f00000000;
	mov.f32 	%f192, %f193;
	@%p23 bra 	$L__BB0_21;

	add.s32 	%r179, %r10, %r216;
	mul.wide.s32 	%rd74, %r179, 4;
	add.s64 	%rd75, %rd1, %rd74;
	ld.global.f32 	%f192, [%rd75];

$L__BB0_21:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f192;}

	// end inline asm
	st.shared.u16 	[%r11+128], %rs14;
	setp.ge.s32 	%p24, %r217, %r54;
	@%p24 bra 	$L__BB0_23;

	add.s32 	%r180, %r10, %r217;
	mul.wide.s32 	%rd76, %r180, 4;
	add.s64 	%rd77, %rd1, %rd76;
	ld.global.f32 	%f193, [%rd77];

$L__BB0_23:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f193;}

	// end inline asm
	st.shared.u16 	[%r11+192], %rs15;
	bar.warp.sync 	-1;
	setp.eq.s64 	%p25, %rd206, 0;
	@%p25 bra 	$L__BB0_99;

	setp.eq.s16 	%p26, %rs96, 0;
	@%p26 bra 	$L__BB0_62;

	setp.eq.s64 	%p27, %rd204, 0;
	@%p27 bra 	$L__BB0_31;

	setp.ne.s16 	%p28, %rs95, 0;
	mov.u32 	%r224, %r18;
	@%p28 bra 	$L__BB0_28;

	mov.u32 	%r224, %r2;

$L__BB0_28:
	mul.wide.u32 	%rd78, %r224, 4;
	add.s64 	%rd79, %rd37, %rd78;
	ld.global.f32 	%f59, [%rd79];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f59;}

	// end inline asm
	st.shared.u16 	[%r16], %rs16;
	setp.eq.s16 	%p29, %rs95, 0;
	@%p29 bra 	$L__BB0_30;

	shl.b64 	%rd80, %rd4, 2;
	add.s64 	%rd81, %rd37, %rd80;
	ld.global.f32 	%f194, [%rd81];
	bra.uni 	$L__BB0_32;

$L__BB0_99:
	setp.eq.s64 	%p57, %rd204, 0;
	@%p57 bra 	$L__BB0_115;

	setp.ne.s16 	%p58, %rs95, 0;
	setp.eq.s16 	%p59, %rs95, 0;
	selp.b32 	%r181, %r2, %r18, %p59;
	mul.wide.u32 	%rd170, %r181, 4;
	add.s64 	%rd171, %rd37, %rd170;
	ld.global.f32 	%f117, [%rd171];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs60, %f117;}

	// end inline asm
	st.shared.u16 	[%r16], %rs60;
	mov.u64 	%rd207, %rd4;
	@%p58 bra 	$L__BB0_102;

	mov.u64 	%rd207, %rd3;

$L__BB0_102:
	shl.b64 	%rd172, %rd207, 2;
	add.s64 	%rd173, %rd37, %rd172;
	ld.global.f32 	%f118, [%rd173];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs61, %f118;}

	// end inline asm
	st.shared.u16 	[%r16+64], %rs61;
	mov.u64 	%rd208, %rd6;
	@%p58 bra 	$L__BB0_104;

	mov.u64 	%rd208, %rd5;

$L__BB0_104:
	shl.b64 	%rd174, %rd208, 2;
	add.s64 	%rd175, %rd37, %rd174;
	ld.global.f32 	%f119, [%rd175];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs62, %f119;}

	// end inline asm
	st.shared.u16 	[%r16+128], %rs62;
	mov.u64 	%rd209, %rd8;
	@%p58 bra 	$L__BB0_106;

	mov.u64 	%rd209, %rd7;

$L__BB0_106:
	shl.b64 	%rd176, %rd209, 2;
	add.s64 	%rd177, %rd37, %rd176;
	ld.global.f32 	%f120, [%rd177];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs63, %f120;}

	// end inline asm
	st.shared.u16 	[%r16+192], %rs63;
	mov.u64 	%rd210, %rd10;
	@%p58 bra 	$L__BB0_108;

	mov.u64 	%rd210, %rd9;

$L__BB0_108:
	shl.b64 	%rd178, %rd210, 2;
	add.s64 	%rd179, %rd37, %rd178;
	ld.global.f32 	%f121, [%rd179];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs64, %f121;}

	// end inline asm
	st.shared.u16 	[%r16+256], %rs64;
	mov.u64 	%rd211, %rd12;
	@%p58 bra 	$L__BB0_110;

	mov.u64 	%rd211, %rd11;

$L__BB0_110:
	shl.b64 	%rd180, %rd211, 2;
	add.s64 	%rd181, %rd37, %rd180;
	ld.global.f32 	%f122, [%rd181];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs65, %f122;}

	// end inline asm
	st.shared.u16 	[%r16+320], %rs65;
	mov.u64 	%rd212, %rd16;
	@%p58 bra 	$L__BB0_112;

	mov.u64 	%rd212, %rd15;

$L__BB0_112:
	shl.b64 	%rd182, %rd212, 2;
	add.s64 	%rd183, %rd37, %rd182;
	ld.global.f32 	%f123, [%rd183];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs66, %f123;}

	// end inline asm
	st.shared.u16 	[%r16+384], %rs66;
	mov.u64 	%rd213, %rd20;
	@%p58 bra 	$L__BB0_114;

	mov.u64 	%rd213, %rd19;

$L__BB0_114:
	shl.b64 	%rd184, %rd213, 2;
	add.s64 	%rd185, %rd37, %rd184;
	ld.global.f32 	%f124, [%rd185];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs67, %f124;}

	// end inline asm
	st.shared.u16 	[%r16+448], %rs67;
	mov.f32 	%f132, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs68, %f132;}

	// end inline asm
	st.shared.u16 	[%r16+512], %rs68;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs69, %f132;}

	// end inline asm
	st.shared.u16 	[%r16+576], %rs69;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs70, %f132;}

	// end inline asm
	st.shared.u16 	[%r16+640], %rs70;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs71, %f132;}

	// end inline asm
	st.shared.u16 	[%r16+704], %rs71;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f132;}

	// end inline asm
	st.shared.u16 	[%r16+768], %rs72;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs73, %f132;}

	// end inline asm
	st.shared.u16 	[%r16+832], %rs73;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs74, %f132;}

	// end inline asm
	st.shared.u16 	[%r16+896], %rs74;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs75, %f132;}

	// end inline asm
	st.shared.u16 	[%r16+960], %rs75;
	bra.uni 	$L__BB0_116;

$L__BB0_62:
	setp.eq.s64 	%p42, %rd204, 0;
	@%p42 bra 	$L__BB0_68;

	setp.ne.s16 	%p43, %rs95, 0;
	mov.u32 	%r225, %r18;
	@%p43 bra 	$L__BB0_65;

	mov.u32 	%r225, %r2;

$L__BB0_65:
	mul.wide.u32 	%rd124, %r225, 4;
	add.s64 	%rd125, %rd37, %rd124;
	ld.global.f32 	%f88, [%rd125];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs38, %f88;}

	// end inline asm
	st.shared.u16 	[%r16], %rs38;
	setp.eq.s16 	%p44, %rs95, 0;
	@%p44 bra 	$L__BB0_67;

	shl.b64 	%rd126, %rd4, 2;
	add.s64 	%rd127, %rd37, %rd126;
	ld.global.f32 	%f201, [%rd127];
	bra.uni 	$L__BB0_69;

$L__BB0_31:
	mov.f32 	%f194, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs17, %f194;}

	// end inline asm
	st.shared.u16 	[%r16], %rs17;
	bra.uni 	$L__BB0_32;

$L__BB0_115:
	mov.f32 	%f148, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs76, %f148;}

	// end inline asm
	st.shared.u16 	[%r16], %rs76;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs77, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+64], %rs77;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs78, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+128], %rs78;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs79, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+192], %rs79;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs80, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+256], %rs80;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs81, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+320], %rs81;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs82, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+384], %rs82;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs83, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+448], %rs83;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs84, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+512], %rs84;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs85, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+576], %rs85;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs86, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+640], %rs86;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs87, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+704], %rs87;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs88, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+768], %rs88;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs89, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+832], %rs89;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs90, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+896], %rs90;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs91, %f148;}

	// end inline asm
	st.shared.u16 	[%r16+960], %rs91;
	bra.uni 	$L__BB0_116;

$L__BB0_30:
	shl.b64 	%rd82, %rd3, 2;
	add.s64 	%rd83, %rd37, %rd82;
	ld.global.f32 	%f194, [%rd83];

$L__BB0_32:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f194;}

	// end inline asm
	st.shared.u16 	[%r16+64], %rs18;
	@%p27 bra 	$L__BB0_41;

	setp.eq.s16 	%p31, %rs95, 0;
	@%p31 bra 	$L__BB0_35;

	shl.b64 	%rd84, %rd6, 2;
	add.s64 	%rd85, %rd37, %rd84;
	ld.global.f32 	%f195, [%rd85];
	bra.uni 	$L__BB0_36;

$L__BB0_41:
	mov.f32 	%f196, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f196;}

	// end inline asm
	st.shared.u16 	[%r16+128], %rs21;
	bra.uni 	$L__BB0_42;

$L__BB0_35:
	shl.b64 	%rd86, %rd5, 2;
	add.s64 	%rd87, %rd37, %rd86;
	ld.global.f32 	%f195, [%rd87];

$L__BB0_36:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f195;}

	// end inline asm
	st.shared.u16 	[%r16+128], %rs19;
	@%p27 bra 	$L__BB0_40;

	setp.eq.s16 	%p68, %rs95, 0;
	@%p68 bra 	$L__BB0_39;

	shl.b64 	%rd88, %rd8, 2;
	add.s64 	%rd89, %rd37, %rd88;
	ld.global.f32 	%f196, [%rd89];
	bra.uni 	$L__BB0_42;

$L__BB0_40:
	mov.f32 	%f197, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs20, %f197;}

	// end inline asm
	st.shared.u16 	[%r16+192], %rs20;
	bra.uni 	$L__BB0_46;

$L__BB0_68:
	mov.f32 	%f201, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs39, %f201;}

	// end inline asm
	st.shared.u16 	[%r16], %rs39;
	bra.uni 	$L__BB0_69;

$L__BB0_39:
	shl.b64 	%rd90, %rd7, 2;
	add.s64 	%rd91, %rd37, %rd90;
	ld.global.f32 	%f196, [%rd91];

$L__BB0_42:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f196;}

	// end inline asm
	st.shared.u16 	[%r16+192], %rs22;
	@%p27 bra 	$L__BB0_51;

	setp.eq.s16 	%p35, %rs95, 0;
	@%p35 bra 	$L__BB0_45;

	shl.b64 	%rd92, %rd10, 2;
	add.s64 	%rd93, %rd37, %rd92;
	ld.global.f32 	%f197, [%rd93];
	bra.uni 	$L__BB0_46;

$L__BB0_51:
	mov.f32 	%f198, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs25, %f198;}

	// end inline asm
	st.shared.u16 	[%r16+256], %rs25;
	bra.uni 	$L__BB0_52;

$L__BB0_45:
	shl.b64 	%rd94, %rd9, 2;
	add.s64 	%rd95, %rd37, %rd94;
	ld.global.f32 	%f197, [%rd95];

$L__BB0_46:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs23, %f197;}

	// end inline asm
	st.shared.u16 	[%r16+256], %rs23;
	@%p27 bra 	$L__BB0_50;

	setp.eq.s16 	%p37, %rs95, 0;
	@%p37 bra 	$L__BB0_49;

	shl.b64 	%rd96, %rd12, 2;
	add.s64 	%rd97, %rd37, %rd96;
	ld.global.f32 	%f198, [%rd97];
	bra.uni 	$L__BB0_52;

$L__BB0_50:
	mov.f32 	%f199, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f199;}

	// end inline asm
	st.shared.u16 	[%r16+320], %rs24;
	bra.uni 	$L__BB0_56;

$L__BB0_49:
	shl.b64 	%rd98, %rd11, 2;
	add.s64 	%rd99, %rd37, %rd98;
	ld.global.f32 	%f198, [%rd99];

$L__BB0_52:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs26, %f198;}

	// end inline asm
	st.shared.u16 	[%r16+320], %rs26;
	@%p27 bra 	$L__BB0_60;

	setp.eq.s16 	%p39, %rs95, 0;
	@%p39 bra 	$L__BB0_55;

	shl.b64 	%rd100, %rd16, 2;
	add.s64 	%rd101, %rd37, %rd100;
	ld.global.f32 	%f199, [%rd101];
	bra.uni 	$L__BB0_56;

$L__BB0_60:
	mov.f32 	%f200, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs28, %f200;}

	// end inline asm
	st.shared.u16 	[%r16+384], %rs28;
	bra.uni 	$L__BB0_61;

$L__BB0_55:
	shl.b64 	%rd102, %rd15, 2;
	add.s64 	%rd103, %rd37, %rd102;
	ld.global.f32 	%f199, [%rd103];

$L__BB0_56:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs27, %f199;}

	// end inline asm
	st.shared.u16 	[%r16+384], %rs27;
	mov.f32 	%f200, 0f00000000;
	@%p27 bra 	$L__BB0_61;

	setp.eq.s16 	%p41, %rs95, 0;
	@%p41 bra 	$L__BB0_59;

	shl.b64 	%rd104, %rd20, 2;
	add.s64 	%rd105, %rd37, %rd104;
	ld.global.f32 	%f200, [%rd105];
	bra.uni 	$L__BB0_61;

$L__BB0_59:
	shl.b64 	%rd106, %rd19, 2;
	add.s64 	%rd107, %rd37, %rd106;
	ld.global.f32 	%f200, [%rd107];

$L__BB0_61:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f200;}

	// end inline asm
	st.shared.u16 	[%r16+448], %rs29;
	add.s64 	%rd109, %rd205, %rd108;
	ld.global.f32 	%f80, [%rd109];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f80;}

	// end inline asm
	st.shared.u16 	[%r16+512], %rs30;
	add.s64 	%rd111, %rd205, %rd110;
	ld.global.f32 	%f81, [%rd111];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs31, %f81;}

	// end inline asm
	st.shared.u16 	[%r16+576], %rs31;
	add.s64 	%rd113, %rd205, %rd112;
	ld.global.f32 	%f82, [%rd113];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs32, %f82;}

	// end inline asm
	st.shared.u16 	[%r16+640], %rs32;
	add.s64 	%rd115, %rd205, %rd114;
	ld.global.f32 	%f83, [%rd115];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs33, %f83;}

	// end inline asm
	st.shared.u16 	[%r16+704], %rs33;
	add.s64 	%rd117, %rd205, %rd116;
	ld.global.f32 	%f84, [%rd117];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs34, %f84;}

	// end inline asm
	st.shared.u16 	[%r16+768], %rs34;
	add.s64 	%rd119, %rd205, %rd118;
	ld.global.f32 	%f85, [%rd119];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs35, %f85;}

	// end inline asm
	st.shared.u16 	[%r16+832], %rs35;
	add.s64 	%rd121, %rd205, %rd120;
	ld.global.f32 	%f86, [%rd121];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs36, %f86;}

	// end inline asm
	st.shared.u16 	[%r16+896], %rs36;
	add.s64 	%rd123, %rd205, %rd122;
	ld.global.f32 	%f87, [%rd123];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs37, %f87;}

	// end inline asm
	st.shared.u16 	[%r16+960], %rs37;
	bra.uni 	$L__BB0_116;

$L__BB0_67:
	shl.b64 	%rd128, %rd3, 2;
	add.s64 	%rd129, %rd37, %rd128;
	ld.global.f32 	%f201, [%rd129];

$L__BB0_69:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs40, %f201;}

	// end inline asm
	st.shared.u16 	[%r16+64], %rs40;
	@%p42 bra 	$L__BB0_78;

	setp.eq.s16 	%p46, %rs95, 0;
	@%p46 bra 	$L__BB0_72;

	shl.b64 	%rd130, %rd6, 2;
	add.s64 	%rd131, %rd37, %rd130;
	ld.global.f32 	%f202, [%rd131];
	bra.uni 	$L__BB0_73;

$L__BB0_78:
	mov.f32 	%f203, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs43, %f203;}

	// end inline asm
	st.shared.u16 	[%r16+128], %rs43;
	bra.uni 	$L__BB0_79;

$L__BB0_72:
	shl.b64 	%rd132, %rd5, 2;
	add.s64 	%rd133, %rd37, %rd132;
	ld.global.f32 	%f202, [%rd133];

$L__BB0_73:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs41, %f202;}

	// end inline asm
	st.shared.u16 	[%r16+128], %rs41;
	@%p42 bra 	$L__BB0_77;

	setp.eq.s16 	%p69, %rs95, 0;
	@%p69 bra 	$L__BB0_76;

	shl.b64 	%rd134, %rd8, 2;
	add.s64 	%rd135, %rd37, %rd134;
	ld.global.f32 	%f203, [%rd135];
	bra.uni 	$L__BB0_79;

$L__BB0_77:
	mov.f32 	%f204, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs42, %f204;}

	// end inline asm
	st.shared.u16 	[%r16+192], %rs42;
	bra.uni 	$L__BB0_83;

$L__BB0_76:
	shl.b64 	%rd136, %rd7, 2;
	add.s64 	%rd137, %rd37, %rd136;
	ld.global.f32 	%f203, [%rd137];

$L__BB0_79:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs44, %f203;}

	// end inline asm
	st.shared.u16 	[%r16+192], %rs44;
	@%p42 bra 	$L__BB0_88;

	setp.eq.s16 	%p50, %rs95, 0;
	@%p50 bra 	$L__BB0_82;

	shl.b64 	%rd138, %rd10, 2;
	add.s64 	%rd139, %rd37, %rd138;
	ld.global.f32 	%f204, [%rd139];
	bra.uni 	$L__BB0_83;

$L__BB0_88:
	mov.f32 	%f205, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs47, %f205;}

	// end inline asm
	st.shared.u16 	[%r16+256], %rs47;
	bra.uni 	$L__BB0_89;

$L__BB0_82:
	shl.b64 	%rd140, %rd9, 2;
	add.s64 	%rd141, %rd37, %rd140;
	ld.global.f32 	%f204, [%rd141];

$L__BB0_83:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs45, %f204;}

	// end inline asm
	st.shared.u16 	[%r16+256], %rs45;
	@%p42 bra 	$L__BB0_87;

	setp.eq.s16 	%p52, %rs95, 0;
	@%p52 bra 	$L__BB0_86;

	shl.b64 	%rd142, %rd12, 2;
	add.s64 	%rd143, %rd37, %rd142;
	ld.global.f32 	%f205, [%rd143];
	bra.uni 	$L__BB0_89;

$L__BB0_87:
	mov.f32 	%f206, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs46, %f206;}

	// end inline asm
	st.shared.u16 	[%r16+320], %rs46;
	bra.uni 	$L__BB0_93;

$L__BB0_86:
	shl.b64 	%rd144, %rd11, 2;
	add.s64 	%rd145, %rd37, %rd144;
	ld.global.f32 	%f205, [%rd145];

$L__BB0_89:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs48, %f205;}

	// end inline asm
	st.shared.u16 	[%r16+320], %rs48;
	@%p42 bra 	$L__BB0_97;

	setp.eq.s16 	%p54, %rs95, 0;
	@%p54 bra 	$L__BB0_92;

	shl.b64 	%rd146, %rd16, 2;
	add.s64 	%rd147, %rd37, %rd146;
	ld.global.f32 	%f206, [%rd147];
	bra.uni 	$L__BB0_93;

$L__BB0_97:
	mov.f32 	%f207, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs50, %f207;}

	// end inline asm
	st.shared.u16 	[%r16+384], %rs50;
	bra.uni 	$L__BB0_98;

$L__BB0_92:
	shl.b64 	%rd148, %rd15, 2;
	add.s64 	%rd149, %rd37, %rd148;
	ld.global.f32 	%f206, [%rd149];

$L__BB0_93:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs49, %f206;}

	// end inline asm
	st.shared.u16 	[%r16+384], %rs49;
	mov.f32 	%f207, 0f00000000;
	@%p42 bra 	$L__BB0_98;

	setp.eq.s16 	%p56, %rs95, 0;
	@%p56 bra 	$L__BB0_96;

	shl.b64 	%rd150, %rd20, 2;
	add.s64 	%rd151, %rd37, %rd150;
	ld.global.f32 	%f207, [%rd151];
	bra.uni 	$L__BB0_98;

$L__BB0_96:
	shl.b64 	%rd152, %rd19, 2;
	add.s64 	%rd153, %rd37, %rd152;
	ld.global.f32 	%f207, [%rd153];

$L__BB0_98:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs51, %f207;}

	// end inline asm
	st.shared.u16 	[%r16+448], %rs51;
	add.s64 	%rd155, %rd205, %rd154;
	ld.global.f32 	%f109, [%rd155];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs52, %f109;}

	// end inline asm
	st.shared.u16 	[%r16+512], %rs52;
	add.s64 	%rd157, %rd205, %rd156;
	ld.global.f32 	%f110, [%rd157];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs53, %f110;}

	// end inline asm
	st.shared.u16 	[%r16+576], %rs53;
	add.s64 	%rd159, %rd205, %rd158;
	ld.global.f32 	%f111, [%rd159];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs54, %f111;}

	// end inline asm
	st.shared.u16 	[%r16+640], %rs54;
	add.s64 	%rd161, %rd205, %rd160;
	ld.global.f32 	%f112, [%rd161];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs55, %f112;}

	// end inline asm
	st.shared.u16 	[%r16+704], %rs55;
	add.s64 	%rd163, %rd205, %rd162;
	ld.global.f32 	%f113, [%rd163];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs56, %f113;}

	// end inline asm
	st.shared.u16 	[%r16+768], %rs56;
	add.s64 	%rd165, %rd205, %rd164;
	ld.global.f32 	%f114, [%rd165];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs57, %f114;}

	// end inline asm
	st.shared.u16 	[%r16+832], %rs57;
	add.s64 	%rd167, %rd205, %rd166;
	ld.global.f32 	%f115, [%rd167];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs58, %f115;}

	// end inline asm
	st.shared.u16 	[%r16+896], %rs58;
	add.s64 	%rd169, %rd205, %rd168;
	ld.global.f32 	%f116, [%rd169];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs59, %f116;}

	// end inline asm
	st.shared.u16 	[%r16+960], %rs59;

$L__BB0_116:
	mov.u32 	%r212, _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE6temp_c;
	mov.u32 	%r211, _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE8shared_b;
	mov.u32 	%r210, _ZZ54batch8_symmetric_block_pair_multiply_wmma_experimentalE8shared_a;
	bar.warp.sync 	-1;
	mov.u32 	%r182, 16;
	wmma.load.a.sync.aligned.row.m32n8k16.shared.f16 	{%r184, %r185, %r186, %r187, %r188, %r189, %r190, %r191}, [%r210], %r182;
	mov.u32 	%r192, 8;
	wmma.load.b.sync.aligned.row.m32n8k16.shared.f16 	{%r194, %r195, %r196, %r197, %r198, %r199, %r200, %r201}, [%r211], %r192;
	mov.f32 	%f149, 0f00000000;
	wmma.mma.sync.aligned.row.row.m32n8k16.f32.f32 {%f150, %f151, %f152, %f153, %f154, %f155, %f156, %f157}, {%r184, %r185, %r186, %r187, %r188, %r189, %r190, %r191}, {%r194, %r195, %r196, %r197, %r198, %r199, %r200, %r201}, {%f149, %f149, %f149, %f149, %f149, %f149, %f149, %f149};
	wmma.store.d.sync.aligned.row.m32n8k16.shared.f32 	[%r212], {%f150, %f151, %f152, %f153, %f154, %f155, %f156, %f157}, %r192;
	bar.warp.sync 	-1;
	ld.shared.f32 	%f158, [%r6];
	ld.shared.f32 	%f159, [%r15];
	add.ftz.f32 	%f160, %f159, %f158;
	st.shared.f32 	[%r6], %f160;
	ld.shared.f32 	%f161, [%r6+4];
	ld.shared.f32 	%f162, [%r15+4];
	add.ftz.f32 	%f163, %f162, %f161;
	st.shared.f32 	[%r6+4], %f163;
	ld.shared.f32 	%f164, [%r6+8];
	ld.shared.f32 	%f165, [%r15+8];
	add.ftz.f32 	%f166, %f165, %f164;
	st.shared.f32 	[%r6+8], %f166;
	ld.shared.f32 	%f167, [%r6+12];
	ld.shared.f32 	%f168, [%r15+12];
	add.ftz.f32 	%f169, %f168, %f167;
	st.shared.f32 	[%r6+12], %f169;
	ld.shared.f32 	%f170, [%r6+16];
	ld.shared.f32 	%f171, [%r15+16];
	add.ftz.f32 	%f172, %f171, %f170;
	st.shared.f32 	[%r6+16], %f172;
	ld.shared.f32 	%f173, [%r6+20];
	ld.shared.f32 	%f174, [%r15+20];
	add.ftz.f32 	%f175, %f174, %f173;
	st.shared.f32 	[%r6+20], %f175;
	ld.shared.f32 	%f176, [%r6+24];
	ld.shared.f32 	%f177, [%r15+24];
	add.ftz.f32 	%f178, %f177, %f176;
	st.shared.f32 	[%r6+24], %f178;
	ld.shared.f32 	%f179, [%r6+28];
	ld.shared.f32 	%f180, [%r15+28];
	add.ftz.f32 	%f181, %f180, %f179;
	st.shared.f32 	[%r6+28], %f181;
	bar.warp.sync 	-1;

$L__BB0_117:
	shl.b32 	%r206, %r52, 8;
	add.s32 	%r223, %r223, 1;
	add.s32 	%r222, %r222, %r206;
	add.s32 	%r221, %r221, 1;
	add.s32 	%r220, %r220, %r206;
	add.s32 	%r219, %r219, %r23;
	add.s32 	%r218, %r218, %r23;
	add.s32 	%r217, %r217, 16;
	add.s32 	%r216, %r216, 16;
	add.s32 	%r215, %r215, 16;
	add.s32 	%r214, %r214, 16;
	setp.lt.s32 	%p66, %r223, %r52;
	@%p66 bra 	$L__BB0_3;

$L__BB0_118:
	mov.u32 	%r207, %ctaid.y;
	shl.b32 	%r203, %r207, 5;
	or.b32  	%r50, %r203, %r2;
	setp.ge.s32 	%p67, %r50, %r54;
	@%p67 bra 	$L__BB0_120;

	ld.param.u64 	%rd200, [batch8_symmetric_block_pair_multiply_wmma_experimental_param_2];
	mov.u32 	%r209, %ctaid.x;
	ld.param.u32 	%r208, [batch8_symmetric_block_pair_multiply_wmma_experimental_param_4];
	ld.shared.f32 	%f182, [%r6];
	mad.lo.s32 	%r204, %r209, %r54, %r50;
	cvta.to.global.u64 	%rd189, %rd200;
	mul.wide.s32 	%rd190, %r204, 4;
	add.s64 	%rd191, %rd189, %rd190;
	st.global.f32 	[%rd191], %f182;
	ld.shared.f32 	%f183, [%r6+4];
	mul.lo.s32 	%r205, %r54, %r208;
	mul.wide.s32 	%rd192, %r205, 4;
	add.s64 	%rd193, %rd191, %rd192;
	st.global.f32 	[%rd193], %f183;
	ld.shared.f32 	%f184, [%r6+8];
	add.s64 	%rd194, %rd193, %rd192;
	st.global.f32 	[%rd194], %f184;
	ld.shared.f32 	%f185, [%r6+12];
	add.s64 	%rd195, %rd194, %rd192;
	st.global.f32 	[%rd195], %f185;
	ld.shared.f32 	%f186, [%r6+16];
	add.s64 	%rd196, %rd195, %rd192;
	st.global.f32 	[%rd196], %f186;
	ld.shared.f32 	%f187, [%r6+20];
	add.s64 	%rd197, %rd196, %rd192;
	st.global.f32 	[%rd197], %f187;
	ld.shared.f32 	%f188, [%r6+24];
	add.s64 	%rd198, %rd197, %rd192;
	st.global.f32 	[%rd198], %f188;
	ld.shared.f32 	%f189, [%r6+28];
	add.s64 	%rd199, %rd198, %rd192;
	st.global.f32 	[%rd199], %f189;

$L__BB0_120:
	ret;

}
